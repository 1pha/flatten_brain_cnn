{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T05:35:58.324803Z",
     "start_time": "2020-07-10T05:35:56.540048Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T05:35:58.327668Z",
     "start_time": "2020-07-10T05:35:58.325649Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read-in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T05:35:59.127738Z",
     "start_time": "2020-07-10T05:35:59.100214Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = '../res/npy_img/left/'\n",
    "files = glob(PATH+'*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T05:38:16.068631Z",
     "start_time": "2020-07-10T05:38:16.036211Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = torch.tensor(np.load(files[0]))\n",
    "x, y, f = np.load(files[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:10:40.976790Z",
     "start_time": "2020-07-10T11:10:40.971833Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_loader(files):\n",
    "    out = []\n",
    "    x, y, f = np.load(files[0]).shape\n",
    "    for file in tqdm(files):\n",
    "        file = np.load(file)\n",
    "        out.append(file.transpose(2, 0, 1))\n",
    "    out = np.array(out)\n",
    "    return torch.tensor(out, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:10:41.311042Z",
     "start_time": "2020-07-10T11:10:41.290848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307, 375, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(files[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:31.356425Z",
     "start_time": "2020-07-10T11:10:44.235050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632c3fdd45e94eb58d06e8d1319a7f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=867.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = data_loader(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:34.120471Z",
     "start_time": "2020-07-10T11:11:33.805674Z"
    }
   },
   "outputs": [],
   "source": [
    "label = pd.read_csv('../res/age_balanced.csv', index_col=0)\n",
    "y_multi = torch.tensor(label.multiclass.values)\n",
    "y_binary = torch.tensor(label.binary.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:35.278354Z",
     "start_time": "2020-07-10T11:11:34.806602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Binary\n",
    "X_train, X_test, y_binary_train, y_binary_test = train_test_split(X, y_binary,\n",
    "                                                                  test_size=0.2, random_state=SEED)\n",
    "train_binary_ds = TensorDataset(X_train, y_binary_train)\n",
    "test_binary_ds = TensorDataset(X_test, y_binary_test)\n",
    "train_binary_loader = DataLoader(train_binary_ds, batch_size=64, shuffle=True)\n",
    "test_binary_loader = DataLoader(test_binary_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "sample_multi_ds = TensorDataset(X_train[2:4], y_binary_train[2:4])\n",
    "sample_multi_loader = DataLoader(sample_multi_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:50.671541Z",
     "start_time": "2020-07-10T11:11:50.066653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Binary\n",
    "X_train, X_test, y_multi_train, y_multi_test = train_test_split(X, y_multi,\n",
    "                                                                test_size=0.2, random_state=SEED)\n",
    "train_multi_ds = TensorDataset(X_train, y_multi_train)\n",
    "test_multi_ds = TensorDataset(X_test, y_multi_test)\n",
    "train_multi_loader = DataLoader(train_multi_ds, batch_size=128, shuffle=True)\n",
    "test_multi_loader = DataLoader(test_multi_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "sample_binary_ds = TensorDataset(X_train[2:4], y_binary_train[2:4])\n",
    "sample_binary_loader = DataLoader(sample_binary_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions in Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:17:36.875385Z",
     "start_time": "2020-07-08T06:17:36.871395Z"
    }
   },
   "outputs": [],
   "source": [
    "def count(y_pred, y_true):\n",
    "    y_pred_ = y_pred.argmax(axis=1)\n",
    "    \n",
    "    corr = 0\n",
    "    for p, t in zip(y_pred_, y_true):\n",
    "        if p == t:\n",
    "            corr += 1\n",
    "            \n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:38.326537Z",
     "start_time": "2020-07-10T11:11:38.306591Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, ctype):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.ctype = ctype\n",
    "        if self.ctype=='binary':\n",
    "            out_node = 1\n",
    "            self.last_layer = F.sigmoid\n",
    "        elif self.ctype=='multi':\n",
    "            out_node = 10\n",
    "            self.last_layer = F.softmax\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(6, 10, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(10, 20, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(20, 40, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        \n",
    "        self.fc1 = nn.Linear(480, 100)\n",
    "        self.fc2 = nn.Linear(100, out_node)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        #out = self.fc3(out)\n",
    "        out = self.last_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions need in Future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:40.390983Z",
     "start_time": "2020-07-10T11:11:40.385997Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_multi(pred, true):\n",
    "    result = 0\n",
    "    cnt = 0\n",
    "    for p, t in zip(pred.argmax(axis=1), true):\n",
    "        cnt += 1\n",
    "        if p == t:\n",
    "            result += 1\n",
    "            \n",
    "    return result, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:40.644345Z",
     "start_time": "2020-07-10T11:11:40.635369Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_binary(pred, true):\n",
    "    correct, cnt = 0, 0\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for p, t in zip(pred, true):\n",
    "        cnt += 1\n",
    "        if (p >= 0.5) & (t == 1):\n",
    "            correct += 1\n",
    "            tp += 1\n",
    "            \n",
    "        elif (p >= 0.5) & (t == 0):\n",
    "            fp += 1\n",
    "            \n",
    "        elif (p < 0.5) & (t == 1):\n",
    "            tn += 1\n",
    "            \n",
    "        elif (p < 0.5) & (t == 0):\n",
    "            correct += 1\n",
    "            fn += 1\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return correct, cnt, np.array([tp, tn, fp, fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:42.047015Z",
     "start_time": "2020-07-10T11:11:42.043015Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_binary(data):\n",
    "    result = []\n",
    "    for d in data:\n",
    "        if d >= 0.5:\n",
    "            result.append(1)\n",
    "        elif d < 0.5:\n",
    "            result.append(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:43.553600Z",
     "start_time": "2020-07-10T11:11:43.409989Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_binary_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-5652a04110fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_binary_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_binary_loader' is not defined"
     ]
    }
   ],
   "source": [
    "conv = ConvNet(ctype='binary')\n",
    "for x, y in sample_binary_loader:\n",
    "    print(conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:11:59.837434Z",
     "start_time": "2020-07-10T11:11:55.198769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pha\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 8, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 2, 9, 2, 7, 2, 2,\n",
      "        1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 3, 2, 9, 7, 4, 2, 2,\n",
      "        3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 3, 0, 2, 3, 2, 2, 2, 2, 7, 0, 2, 2, 0,\n",
      "        2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 7, 2, 2, 9, 2, 2, 5, 3,\n",
      "        3, 2, 2, 2, 2, 3, 1, 2], grad_fn=<NotImplemented>)\n",
      "tensor([2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 9, 2, 7, 4, 0, 9, 2, 2, 2, 2, 3, 2,\n",
      "        2, 4, 7, 2, 3, 2, 9, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 5, 2, 0, 2, 3, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 2, 2, 3, 3, 5, 2, 2, 2, 3, 2, 2, 2,\n",
      "        2, 2, 1, 2, 2, 0, 2, 2, 2, 3, 7, 2, 2, 2, 0, 2, 2, 7, 2, 2, 2, 2, 8, 2,\n",
      "        3, 8, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 4, 2, 1, 2, 7, 2, 3, 2, 2, 7, 2,\n",
      "        2, 2, 2, 3, 2, 2, 2, 0], grad_fn=<NotImplemented>)\n",
      "tensor([2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 8, 2, 2, 0,\n",
      "        2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 3, 2, 2, 2, 2, 7, 0, 2, 2, 2, 2, 2, 4, 0,\n",
      "        2, 3, 2, 2, 0, 0, 7, 1, 2, 3, 2, 3, 0, 2, 3, 2, 2, 2, 3, 0, 2, 2, 9, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 4, 0, 2, 2, 2, 2, 2, 2, 2, 3, 2,\n",
      "        2, 2, 2, 2, 7, 0, 2, 2, 2, 2, 2, 2, 9, 5, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3,\n",
      "        7, 2, 2, 0, 2, 2, 2, 0], grad_fn=<NotImplemented>)\n",
      "tensor([2, 2, 0, 2, 2, 0, 2, 2, 3, 2, 0, 0, 2, 3, 3, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 9, 2, 1, 2, 2, 2, 2, 2, 2, 9, 2, 0, 2, 2,\n",
      "        5, 7, 3, 3, 2, 2, 2, 2, 0, 2, 2, 2, 2, 3, 1, 2, 3, 3, 2, 2, 2, 0, 3, 2,\n",
      "        0, 2, 2, 3, 9, 3, 2, 2, 2, 3, 2, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 7,\n",
      "        2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 7, 2, 2, 2, 2, 0,\n",
      "        2, 2, 3, 0, 7, 3, 2, 0], grad_fn=<NotImplemented>)\n",
      "tensor([7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 7,\n",
      "        3, 2, 1, 2, 2, 2, 7, 0, 2, 3, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        0, 0, 7, 2, 2, 3, 0, 2, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        5, 7, 2, 3, 2, 2, 2, 0, 0, 2, 0, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        7, 2, 2, 2, 7, 2, 0, 2, 2, 2, 9, 4, 7, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2], grad_fn=<NotImplemented>)\n",
      "tensor([2, 7, 2, 3, 9, 0, 2, 2, 0, 3, 3, 2, 0, 2, 2, 0, 1, 9, 2, 3, 2, 2, 2, 2,\n",
      "        3, 2, 3, 7, 1, 2, 2, 2, 2, 2, 0, 2, 9, 2, 2, 3, 2, 1, 0, 2, 7, 5, 0, 2,\n",
      "        2, 2, 2, 2, 2], grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "conv = ConvNet(ctype='multi')\n",
    "for x, y in train_multi_loader:\n",
    "    print(conv(x).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T11:12:02.556812Z",
     "start_time": "2020-07-10T11:12:02.543846Z"
    }
   },
   "outputs": [],
   "source": [
    "multi_weight = sorted(Counter(label.multiclass).items(), key=(lambda x: x[0]))\n",
    "multi_weight = 1 / np.array([w[1] for w in multi_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:17:39.932102Z",
     "start_time": "2020-07-08T06:17:39.923618Z"
    }
   },
   "outputs": [],
   "source": [
    "binary_weight = sorted(Counter(label.binary).items(), key=(lambda x: x[0]))\n",
    "binary_weight = 1 / np.array([w[1] for w in binary_weight])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T13:39:35.187604Z",
     "start_time": "2020-07-10T11:51:53.401260Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pha\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "[LOSS] train: 13.839, val: 4.611\n",
      "[ACC%] train: 9.24%, val: 8.05%\n",
      "EPOCH: 1\n",
      "[LOSS] train: 13.834, val: 4.616\n",
      "[ACC%] train: 10.1%, val: 9.2%\n",
      "EPOCH: 2\n",
      "[LOSS] train: 13.811, val: 4.611\n",
      "[ACC%] train: 12.84%, val: 9.2%\n",
      "EPOCH: 3\n",
      "[LOSS] train: 13.811, val: 4.61\n",
      "[ACC%] train: 13.13%, val: 9.2%\n",
      "EPOCH: 4\n",
      "[LOSS] train: 13.812, val: 4.611\n",
      "[ACC%] train: 10.39%, val: 9.2%\n",
      "EPOCH: 5\n",
      "[LOSS] train: 13.813, val: 4.616\n",
      "[ACC%] train: 11.98%, val: 9.2%\n",
      "EPOCH: 6\n",
      "[LOSS] train: 13.812, val: 4.61\n",
      "[ACC%] train: 12.41%, val: 9.2%\n",
      "EPOCH: 7\n",
      "[LOSS] train: 13.812, val: 4.613\n",
      "[ACC%] train: 12.84%, val: 9.2%\n",
      "EPOCH: 8\n",
      "[LOSS] train: 13.813, val: 4.611\n",
      "[ACC%] train: 13.28%, val: 9.2%\n",
      "EPOCH: 9\n",
      "[LOSS] train: 13.805, val: 4.612\n",
      "[ACC%] train: 11.4%, val: 8.05%\n",
      "EPOCH: 10\n",
      "[LOSS] train: 13.81, val: 4.622\n",
      "[ACC%] train: 11.98%, val: 8.05%\n",
      "EPOCH: 11\n",
      "[LOSS] train: 13.805, val: 4.619\n",
      "[ACC%] train: 11.11%, val: 8.05%\n",
      "EPOCH: 12\n",
      "[LOSS] train: 13.805, val: 4.617\n",
      "[ACC%] train: 11.26%, val: 8.05%\n",
      "EPOCH: 13\n",
      "[LOSS] train: 13.807, val: 4.614\n",
      "[ACC%] train: 14.86%, val: 9.2%\n",
      "EPOCH: 14\n",
      "[LOSS] train: 13.804, val: 4.619\n",
      "[ACC%] train: 12.27%, val: 8.62%\n",
      "EPOCH: 15\n",
      "[LOSS] train: 13.807, val: 4.622\n",
      "[ACC%] train: 13.71%, val: 9.2%\n",
      "EPOCH: 16\n",
      "[LOSS] train: 13.8, val: 4.619\n",
      "[ACC%] train: 13.71%, val: 8.62%\n",
      "EPOCH: 17\n",
      "[LOSS] train: 13.799, val: 4.614\n",
      "[ACC%] train: 12.27%, val: 8.05%\n",
      "EPOCH: 18\n",
      "[LOSS] train: 13.805, val: 4.612\n",
      "[ACC%] train: 13.71%, val: 9.77%\n",
      "EPOCH: 19\n",
      "[LOSS] train: 13.799, val: 4.614\n",
      "[ACC%] train: 13.42%, val: 8.62%\n",
      "EPOCH: 20\n",
      "[LOSS] train: 13.794, val: 4.616\n",
      "[ACC%] train: 14.29%, val: 9.2%\n",
      "EPOCH: 21\n",
      "[LOSS] train: 13.796, val: 4.626\n",
      "[ACC%] train: 14.0%, val: 9.2%\n",
      "EPOCH: 22\n",
      "[LOSS] train: 13.789, val: 4.628\n",
      "[ACC%] train: 11.98%, val: 8.05%\n",
      "EPOCH: 23\n",
      "[LOSS] train: 13.799, val: 4.628\n",
      "[ACC%] train: 13.56%, val: 5.75%\n",
      "EPOCH: 24\n",
      "[LOSS] train: 13.781, val: 4.627\n",
      "[ACC%] train: 12.7%, val: 8.05%\n",
      "EPOCH: 25\n",
      "[LOSS] train: 13.778, val: 4.625\n",
      "[ACC%] train: 11.83%, val: 8.62%\n",
      "EPOCH: 26\n",
      "[LOSS] train: 13.777, val: 4.627\n",
      "[ACC%] train: 13.56%, val: 9.2%\n",
      "EPOCH: 27\n",
      "[LOSS] train: 13.757, val: 4.616\n",
      "[ACC%] train: 14.14%, val: 7.47%\n",
      "EPOCH: 28\n",
      "[LOSS] train: 13.768, val: 4.627\n",
      "[ACC%] train: 12.55%, val: 6.32%\n",
      "EPOCH: 29\n",
      "[LOSS] train: 13.772, val: 4.616\n",
      "[ACC%] train: 14.86%, val: 7.47%\n",
      "EPOCH: 30\n",
      "[LOSS] train: 13.719, val: 4.63\n",
      "[ACC%] train: 16.45%, val: 8.05%\n",
      "EPOCH: 31\n",
      "[LOSS] train: 13.767, val: 4.614\n",
      "[ACC%] train: 12.84%, val: 11.49%\n",
      "EPOCH: 32\n",
      "[LOSS] train: 13.769, val: 4.612\n",
      "[ACC%] train: 11.54%, val: 9.2%\n",
      "EPOCH: 33\n",
      "[LOSS] train: 13.709, val: 4.623\n",
      "[ACC%] train: 14.14%, val: 9.2%\n",
      "EPOCH: 34\n",
      "[LOSS] train: 13.668, val: 4.624\n",
      "[ACC%] train: 15.3%, val: 9.2%\n",
      "EPOCH: 35\n",
      "[LOSS] train: 13.673, val: 4.63\n",
      "[ACC%] train: 16.31%, val: 8.62%\n",
      "EPOCH: 36\n",
      "[LOSS] train: 13.63, val: 4.625\n",
      "[ACC%] train: 16.59%, val: 8.62%\n",
      "EPOCH: 37\n",
      "[LOSS] train: 13.635, val: 4.629\n",
      "[ACC%] train: 16.88%, val: 6.9%\n",
      "EPOCH: 38\n",
      "[LOSS] train: 13.578, val: 4.621\n",
      "[ACC%] train: 17.03%, val: 10.34%\n",
      "EPOCH: 39\n",
      "[LOSS] train: 13.599, val: 4.612\n",
      "[ACC%] train: 18.9%, val: 7.47%\n",
      "EPOCH: 40\n",
      "[LOSS] train: 13.53, val: 4.623\n",
      "[ACC%] train: 19.77%, val: 9.2%\n",
      "EPOCH: 41\n",
      "[LOSS] train: 13.457, val: 4.636\n",
      "[ACC%] train: 21.07%, val: 8.62%\n",
      "EPOCH: 42\n",
      "[LOSS] train: 13.308, val: 4.648\n",
      "[ACC%] train: 25.4%, val: 7.47%\n",
      "EPOCH: 43\n",
      "[LOSS] train: 13.252, val: 4.65\n",
      "[ACC%] train: 24.24%, val: 9.77%\n",
      "EPOCH: 44\n",
      "[LOSS] train: 13.25, val: 4.636\n",
      "[ACC%] train: 26.98%, val: 8.62%\n",
      "EPOCH: 45\n",
      "[LOSS] train: 13.221, val: 4.689\n",
      "[ACC%] train: 26.12%, val: 6.9%\n",
      "EPOCH: 46\n",
      "[LOSS] train: 13.167, val: 4.663\n",
      "[ACC%] train: 27.27%, val: 8.62%\n",
      "EPOCH: 47\n",
      "[LOSS] train: 13.108, val: 4.636\n",
      "[ACC%] train: 27.71%, val: 7.47%\n",
      "EPOCH: 48\n",
      "[LOSS] train: 13.144, val: 4.648\n",
      "[ACC%] train: 26.12%, val: 9.2%\n",
      "EPOCH: 49\n",
      "[LOSS] train: 13.017, val: 4.641\n",
      "[ACC%] train: 28.86%, val: 10.92%\n",
      "EPOCH: 50\n",
      "[LOSS] train: 12.809, val: 4.644\n",
      "[ACC%] train: 34.78%, val: 10.34%\n",
      "EPOCH: 51\n",
      "[LOSS] train: 12.798, val: 4.654\n",
      "[ACC%] train: 33.91%, val: 9.2%\n",
      "EPOCH: 52\n",
      "[LOSS] train: 12.658, val: 4.673\n",
      "[ACC%] train: 36.94%, val: 10.34%\n",
      "EPOCH: 53\n",
      "[LOSS] train: 12.67, val: 4.658\n",
      "[ACC%] train: 36.22%, val: 8.62%\n",
      "EPOCH: 54\n",
      "[LOSS] train: 12.699, val: 4.66\n",
      "[ACC%] train: 32.9%, val: 10.34%\n",
      "EPOCH: 55\n",
      "[LOSS] train: 12.549, val: 4.656\n",
      "[ACC%] train: 37.81%, val: 8.62%\n",
      "EPOCH: 56\n",
      "[LOSS] train: 12.568, val: 4.649\n",
      "[ACC%] train: 38.1%, val: 9.2%\n",
      "EPOCH: 57\n",
      "[LOSS] train: 12.512, val: 4.663\n",
      "[ACC%] train: 36.8%, val: 8.05%\n",
      "EPOCH: 58\n",
      "[LOSS] train: 12.404, val: 4.662\n",
      "[ACC%] train: 41.41%, val: 12.07%\n",
      "EPOCH: 59\n",
      "[LOSS] train: 12.294, val: 4.676\n",
      "[ACC%] train: 41.99%, val: 10.34%\n",
      "EPOCH: 60\n",
      "[LOSS] train: 12.115, val: 4.643\n",
      "[ACC%] train: 45.74%, val: 9.77%\n",
      "EPOCH: 61\n",
      "[LOSS] train: 12.203, val: 4.63\n",
      "[ACC%] train: 43.58%, val: 9.77%\n",
      "EPOCH: 62\n",
      "[LOSS] train: 12.015, val: 4.64\n",
      "[ACC%] train: 47.76%, val: 12.07%\n",
      "EPOCH: 63\n",
      "[LOSS] train: 12.12, val: 4.664\n",
      "[ACC%] train: 47.04%, val: 9.77%\n",
      "EPOCH: 64\n",
      "[LOSS] train: 11.888, val: 4.666\n",
      "[ACC%] train: 48.77%, val: 10.34%\n",
      "EPOCH: 65\n",
      "[LOSS] train: 11.865, val: 4.657\n",
      "[ACC%] train: 49.78%, val: 9.77%\n",
      "EPOCH: 66\n",
      "[LOSS] train: 11.825, val: 4.674\n",
      "[ACC%] train: 50.51%, val: 9.77%\n",
      "EPOCH: 67\n",
      "[LOSS] train: 11.773, val: 4.638\n",
      "[ACC%] train: 51.23%, val: 10.34%\n",
      "EPOCH: 68\n",
      "[LOSS] train: 11.855, val: 4.644\n",
      "[ACC%] train: 49.49%, val: 10.34%\n",
      "EPOCH: 69\n",
      "[LOSS] train: 11.8, val: 4.635\n",
      "[ACC%] train: 50.36%, val: 9.77%\n",
      "EPOCH: 70\n",
      "[LOSS] train: 11.886, val: 4.647\n",
      "[ACC%] train: 48.77%, val: 10.92%\n",
      "EPOCH: 71\n",
      "[LOSS] train: 11.757, val: 4.679\n",
      "[ACC%] train: 50.51%, val: 10.92%\n",
      "EPOCH: 72\n",
      "[LOSS] train: 11.72, val: 4.659\n",
      "[ACC%] train: 52.81%, val: 10.34%\n",
      "EPOCH: 73\n",
      "[LOSS] train: 11.773, val: 4.687\n",
      "[ACC%] train: 50.94%, val: 7.47%\n",
      "EPOCH: 74\n",
      "[LOSS] train: 11.638, val: 4.685\n",
      "[ACC%] train: 53.68%, val: 9.2%\n",
      "EPOCH: 75\n",
      "[LOSS] train: 11.561, val: 4.695\n",
      "[ACC%] train: 54.55%, val: 9.77%\n",
      "EPOCH: 76\n",
      "[LOSS] train: 11.518, val: 4.667\n",
      "[ACC%] train: 52.96%, val: 8.05%\n",
      "EPOCH: 77\n",
      "[LOSS] train: 11.559, val: 4.673\n",
      "[ACC%] train: 54.69%, val: 9.2%\n",
      "EPOCH: 78\n",
      "[LOSS] train: 11.632, val: 4.667\n",
      "[ACC%] train: 54.83%, val: 9.2%\n",
      "EPOCH: 79\n",
      "[LOSS] train: 11.634, val: 4.672\n",
      "[ACC%] train: 54.98%, val: 9.2%\n",
      "EPOCH: 80\n",
      "[LOSS] train: 11.412, val: 4.711\n",
      "[ACC%] train: 55.84%, val: 8.05%\n",
      "EPOCH: 81\n",
      "[LOSS] train: 11.347, val: 4.636\n",
      "[ACC%] train: 57.29%, val: 10.92%\n",
      "EPOCH: 82\n",
      "[LOSS] train: 11.403, val: 4.674\n",
      "[ACC%] train: 57.43%, val: 8.05%\n",
      "EPOCH: 83\n",
      "[LOSS] train: 11.344, val: 4.68\n",
      "[ACC%] train: 58.73%, val: 10.34%\n",
      "EPOCH: 84\n",
      "[LOSS] train: 11.358, val: 4.664\n",
      "[ACC%] train: 58.15%, val: 8.62%\n",
      "EPOCH: 85\n",
      "[LOSS] train: 11.232, val: 4.693\n",
      "[ACC%] train: 59.02%, val: 9.2%\n",
      "EPOCH: 86\n",
      "[LOSS] train: 11.227, val: 4.672\n",
      "[ACC%] train: 61.04%, val: 9.77%\n",
      "EPOCH: 87\n",
      "[LOSS] train: 11.302, val: 4.698\n",
      "[ACC%] train: 58.73%, val: 8.05%\n",
      "EPOCH: 88\n",
      "[LOSS] train: 11.3, val: 4.71\n",
      "[ACC%] train: 58.44%, val: 8.05%\n",
      "EPOCH: 89\n",
      "[LOSS] train: 11.19, val: 4.699\n",
      "[ACC%] train: 59.88%, val: 9.2%\n",
      "EPOCH: 90\n",
      "[LOSS] train: 11.166, val: 4.679\n",
      "[ACC%] train: 60.89%, val: 10.34%\n",
      "EPOCH: 91\n",
      "[LOSS] train: 11.118, val: 4.657\n",
      "[ACC%] train: 62.34%, val: 8.62%\n",
      "EPOCH: 92\n",
      "[LOSS] train: 11.177, val: 4.687\n",
      "[ACC%] train: 59.45%, val: 8.62%\n",
      "EPOCH: 93\n",
      "[LOSS] train: 11.101, val: 4.708\n",
      "[ACC%] train: 62.05%, val: 7.47%\n",
      "EPOCH: 94\n",
      "[LOSS] train: 11.158, val: 4.683\n",
      "[ACC%] train: 59.74%, val: 9.77%\n",
      "EPOCH: 95\n",
      "[LOSS] train: 11.059, val: 4.653\n",
      "[ACC%] train: 61.47%, val: 7.47%\n",
      "EPOCH: 96\n",
      "[LOSS] train: 10.991, val: 4.695\n",
      "[ACC%] train: 62.91%, val: 8.05%\n",
      "EPOCH: 97\n",
      "[LOSS] train: 11.047, val: 4.643\n",
      "[ACC%] train: 61.18%, val: 10.92%\n",
      "EPOCH: 98\n",
      "[LOSS] train: 11.044, val: 4.695\n",
      "[ACC%] train: 63.49%, val: 9.2%\n",
      "EPOCH: 99\n",
      "[LOSS] train: 11.064, val: 4.672\n",
      "[ACC%] train: 62.77%, val: 8.05%\n",
      "EPOCH: 100\n",
      "[LOSS] train: 11.018, val: 4.669\n",
      "[ACC%] train: 61.9%, val: 10.34%\n",
      "EPOCH: 101\n",
      "[LOSS] train: 10.933, val: 4.59\n",
      "[ACC%] train: 63.2%, val: 12.64%\n",
      "EPOCH: 102\n",
      "[LOSS] train: 10.996, val: 4.661\n",
      "[ACC%] train: 61.76%, val: 9.77%\n",
      "EPOCH: 103\n",
      "[LOSS] train: 10.902, val: 4.647\n",
      "[ACC%] train: 64.07%, val: 10.34%\n",
      "EPOCH: 104\n",
      "[LOSS] train: 10.9, val: 4.677\n",
      "[ACC%] train: 64.65%, val: 10.34%\n",
      "EPOCH: 105\n",
      "[LOSS] train: 10.961, val: 4.667\n",
      "[ACC%] train: 65.37%, val: 9.77%\n",
      "EPOCH: 106\n",
      "[LOSS] train: 10.846, val: 4.643\n",
      "[ACC%] train: 66.09%, val: 10.34%\n",
      "EPOCH: 107\n",
      "[LOSS] train: 10.853, val: 4.652\n",
      "[ACC%] train: 64.79%, val: 10.92%\n",
      "EPOCH: 108\n",
      "[LOSS] train: 10.88, val: 4.674\n",
      "[ACC%] train: 66.67%, val: 10.34%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 109\n",
      "[LOSS] train: 10.743, val: 4.635\n",
      "[ACC%] train: 67.97%, val: 9.2%\n",
      "EPOCH: 110\n",
      "[LOSS] train: 10.782, val: 4.664\n",
      "[ACC%] train: 68.25%, val: 10.92%\n",
      "EPOCH: 111\n",
      "[LOSS] train: 10.733, val: 4.666\n",
      "[ACC%] train: 68.4%, val: 12.07%\n",
      "EPOCH: 112\n",
      "[LOSS] train: 10.689, val: 4.635\n",
      "[ACC%] train: 69.55%, val: 10.34%\n",
      "EPOCH: 113\n",
      "[LOSS] train: 10.779, val: 4.641\n",
      "[ACC%] train: 69.41%, val: 13.79%\n",
      "EPOCH: 114\n",
      "[LOSS] train: 10.663, val: 4.649\n",
      "[ACC%] train: 68.98%, val: 10.92%\n",
      "EPOCH: 115\n",
      "[LOSS] train: 10.663, val: 4.654\n",
      "[ACC%] train: 70.56%, val: 10.92%\n",
      "EPOCH: 116\n",
      "[LOSS] train: 10.68, val: 4.663\n",
      "[ACC%] train: 69.41%, val: 9.77%\n",
      "EPOCH: 117\n",
      "[LOSS] train: 10.634, val: 4.617\n",
      "[ACC%] train: 71.0%, val: 14.94%\n",
      "EPOCH: 118\n",
      "[LOSS] train: 10.552, val: 4.638\n",
      "[ACC%] train: 71.14%, val: 12.07%\n",
      "EPOCH: 119\n",
      "[LOSS] train: 10.624, val: 4.624\n",
      "[ACC%] train: 71.57%, val: 9.77%\n",
      "EPOCH: 120\n",
      "[LOSS] train: 10.586, val: 4.653\n",
      "[ACC%] train: 71.28%, val: 11.49%\n",
      "EPOCH: 121\n",
      "[LOSS] train: 10.622, val: 4.654\n",
      "[ACC%] train: 71.28%, val: 11.49%\n",
      "EPOCH: 122\n",
      "[LOSS] train: 10.495, val: 4.625\n",
      "[ACC%] train: 72.29%, val: 14.37%\n",
      "EPOCH: 123\n",
      "[LOSS] train: 10.465, val: 4.623\n",
      "[ACC%] train: 74.75%, val: 14.94%\n",
      "EPOCH: 124\n",
      "[LOSS] train: 10.488, val: 4.658\n",
      "[ACC%] train: 74.17%, val: 10.92%\n",
      "EPOCH: 125\n",
      "[LOSS] train: 10.487, val: 4.641\n",
      "[ACC%] train: 74.03%, val: 9.2%\n",
      "EPOCH: 126\n",
      "[LOSS] train: 10.338, val: 4.639\n",
      "[ACC%] train: 76.33%, val: 10.34%\n",
      "EPOCH: 127\n",
      "[LOSS] train: 10.45, val: 4.613\n",
      "[ACC%] train: 73.3%, val: 10.92%\n",
      "EPOCH: 128\n",
      "[LOSS] train: 10.411, val: 4.656\n",
      "[ACC%] train: 74.75%, val: 10.34%\n",
      "EPOCH: 129\n",
      "[LOSS] train: 10.306, val: 4.634\n",
      "[ACC%] train: 77.2%, val: 13.22%\n",
      "EPOCH: 130\n",
      "[LOSS] train: 10.362, val: 4.545\n",
      "[ACC%] train: 76.48%, val: 12.64%\n",
      "EPOCH: 131\n",
      "[LOSS] train: 10.347, val: 4.64\n",
      "[ACC%] train: 76.33%, val: 11.49%\n",
      "EPOCH: 132\n",
      "[LOSS] train: 10.306, val: 4.62\n",
      "[ACC%] train: 76.05%, val: 13.79%\n",
      "EPOCH: 133\n",
      "[LOSS] train: 10.274, val: 4.58\n",
      "[ACC%] train: 75.76%, val: 10.34%\n",
      "EPOCH: 134\n",
      "[LOSS] train: 10.359, val: 4.625\n",
      "[ACC%] train: 75.76%, val: 12.07%\n",
      "EPOCH: 135\n",
      "[LOSS] train: 10.213, val: 4.689\n",
      "[ACC%] train: 77.06%, val: 9.2%\n",
      "EPOCH: 136\n",
      "[LOSS] train: 10.327, val: 4.626\n",
      "[ACC%] train: 76.19%, val: 11.49%\n",
      "EPOCH: 137\n",
      "[LOSS] train: 10.337, val: 4.631\n",
      "[ACC%] train: 75.76%, val: 8.62%\n",
      "EPOCH: 138\n",
      "[LOSS] train: 10.16, val: 4.657\n",
      "[ACC%] train: 78.79%, val: 12.64%\n",
      "EPOCH: 139\n",
      "[LOSS] train: 10.165, val: 4.625\n",
      "[ACC%] train: 77.63%, val: 10.34%\n",
      "EPOCH: 140\n",
      "[LOSS] train: 10.156, val: 4.623\n",
      "[ACC%] train: 79.08%, val: 12.07%\n",
      "EPOCH: 141\n",
      "[LOSS] train: 10.064, val: 4.645\n",
      "[ACC%] train: 79.94%, val: 10.92%\n",
      "EPOCH: 142\n",
      "[LOSS] train: 10.124, val: 4.65\n",
      "[ACC%] train: 78.35%, val: 13.22%\n",
      "EPOCH: 143\n",
      "[LOSS] train: 10.243, val: 4.645\n",
      "[ACC%] train: 75.9%, val: 12.64%\n",
      "EPOCH: 144\n",
      "[LOSS] train: 10.131, val: 4.571\n",
      "[ACC%] train: 79.22%, val: 15.52%\n",
      "EPOCH: 145\n",
      "[LOSS] train: 10.105, val: 4.615\n",
      "[ACC%] train: 78.93%, val: 12.64%\n",
      "EPOCH: 146\n",
      "[LOSS] train: 10.051, val: 4.636\n",
      "[ACC%] train: 78.5%, val: 9.77%\n",
      "EPOCH: 147\n",
      "[LOSS] train: 10.045, val: 4.59\n",
      "[ACC%] train: 79.65%, val: 12.07%\n",
      "EPOCH: 148\n",
      "[LOSS] train: 9.943, val: 4.636\n",
      "[ACC%] train: 81.53%, val: 13.22%\n",
      "EPOCH: 149\n",
      "[LOSS] train: 9.905, val: 4.65\n",
      "[ACC%] train: 81.53%, val: 11.49%\n",
      "EPOCH: 150\n",
      "[LOSS] train: 9.996, val: 4.704\n",
      "[ACC%] train: 80.23%, val: 8.62%\n",
      "EPOCH: 151\n",
      "[LOSS] train: 10.215, val: 4.672\n",
      "[ACC%] train: 79.22%, val: 8.62%\n",
      "EPOCH: 152\n",
      "[LOSS] train: 10.1, val: 4.615\n",
      "[ACC%] train: 78.93%, val: 14.37%\n",
      "EPOCH: 153\n",
      "[LOSS] train: 9.996, val: 4.641\n",
      "[ACC%] train: 81.53%, val: 12.07%\n",
      "EPOCH: 154\n",
      "[LOSS] train: 10.002, val: 4.647\n",
      "[ACC%] train: 80.52%, val: 14.37%\n",
      "EPOCH: 155\n",
      "[LOSS] train: 9.924, val: 4.662\n",
      "[ACC%] train: 82.4%, val: 10.92%\n",
      "EPOCH: 156\n",
      "[LOSS] train: 9.903, val: 4.605\n",
      "[ACC%] train: 82.4%, val: 15.52%\n",
      "EPOCH: 157\n",
      "[LOSS] train: 9.931, val: 4.659\n",
      "[ACC%] train: 81.82%, val: 10.92%\n",
      "EPOCH: 158\n",
      "[LOSS] train: 9.866, val: 4.628\n",
      "[ACC%] train: 82.4%, val: 10.92%\n",
      "EPOCH: 159\n",
      "[LOSS] train: 9.892, val: 4.602\n",
      "[ACC%] train: 83.55%, val: 9.2%\n",
      "EPOCH: 160\n",
      "[LOSS] train: 9.898, val: 4.676\n",
      "[ACC%] train: 83.55%, val: 10.34%\n",
      "EPOCH: 161\n",
      "[LOSS] train: 9.841, val: 4.573\n",
      "[ACC%] train: 83.98%, val: 12.07%\n",
      "EPOCH: 162\n",
      "[LOSS] train: 9.869, val: 4.635\n",
      "[ACC%] train: 83.12%, val: 11.49%\n",
      "EPOCH: 163\n",
      "[LOSS] train: 9.817, val: 4.618\n",
      "[ACC%] train: 83.84%, val: 11.49%\n",
      "EPOCH: 164\n",
      "[LOSS] train: 9.844, val: 4.662\n",
      "[ACC%] train: 83.69%, val: 9.77%\n",
      "EPOCH: 165\n",
      "[LOSS] train: 9.879, val: 4.646\n",
      "[ACC%] train: 83.26%, val: 12.07%\n",
      "EPOCH: 166\n",
      "[LOSS] train: 9.681, val: 4.629\n",
      "[ACC%] train: 87.16%, val: 14.37%\n",
      "EPOCH: 167\n",
      "[LOSS] train: 9.678, val: 4.616\n",
      "[ACC%] train: 86.58%, val: 10.92%\n",
      "EPOCH: 168\n",
      "[LOSS] train: 9.792, val: 4.642\n",
      "[ACC%] train: 84.56%, val: 12.64%\n",
      "EPOCH: 169\n",
      "[LOSS] train: 9.721, val: 4.641\n",
      "[ACC%] train: 86.0%, val: 10.92%\n",
      "EPOCH: 170\n",
      "[LOSS] train: 9.702, val: 4.653\n",
      "[ACC%] train: 87.3%, val: 11.49%\n",
      "EPOCH: 171\n",
      "[LOSS] train: 9.673, val: 4.629\n",
      "[ACC%] train: 87.3%, val: 9.77%\n",
      "EPOCH: 172\n",
      "[LOSS] train: 9.776, val: 4.619\n",
      "[ACC%] train: 83.98%, val: 11.49%\n",
      "EPOCH: 173\n",
      "[LOSS] train: 9.81, val: 4.701\n",
      "[ACC%] train: 84.56%, val: 9.2%\n",
      "EPOCH: 174\n",
      "[LOSS] train: 9.598, val: 4.658\n",
      "[ACC%] train: 87.45%, val: 10.92%\n",
      "EPOCH: 175\n",
      "[LOSS] train: 9.607, val: 4.646\n",
      "[ACC%] train: 88.17%, val: 12.64%\n",
      "EPOCH: 176\n",
      "[LOSS] train: 9.571, val: 4.617\n",
      "[ACC%] train: 87.88%, val: 12.64%\n",
      "EPOCH: 177\n",
      "[LOSS] train: 9.655, val: 4.619\n",
      "[ACC%] train: 87.16%, val: 13.22%\n",
      "EPOCH: 178\n",
      "[LOSS] train: 9.712, val: 4.639\n",
      "[ACC%] train: 85.43%, val: 13.22%\n",
      "EPOCH: 179\n",
      "[LOSS] train: 9.616, val: 4.629\n",
      "[ACC%] train: 86.72%, val: 12.07%\n",
      "EPOCH: 180\n",
      "[LOSS] train: 9.576, val: 4.595\n",
      "[ACC%] train: 88.31%, val: 13.22%\n",
      "EPOCH: 181\n",
      "[LOSS] train: 9.498, val: 4.608\n",
      "[ACC%] train: 89.03%, val: 13.22%\n",
      "EPOCH: 182\n",
      "[LOSS] train: 9.501, val: 4.685\n",
      "[ACC%] train: 89.75%, val: 12.07%\n",
      "EPOCH: 183\n",
      "[LOSS] train: 9.594, val: 4.62\n",
      "[ACC%] train: 88.02%, val: 13.79%\n",
      "EPOCH: 184\n",
      "[LOSS] train: 9.734, val: 4.671\n",
      "[ACC%] train: 85.86%, val: 9.77%\n",
      "EPOCH: 185\n",
      "[LOSS] train: 9.494, val: 4.663\n",
      "[ACC%] train: 88.89%, val: 12.07%\n",
      "EPOCH: 186\n",
      "[LOSS] train: 9.689, val: 4.697\n",
      "[ACC%] train: 85.71%, val: 8.05%\n",
      "EPOCH: 187\n",
      "[LOSS] train: 9.739, val: 4.645\n",
      "[ACC%] train: 85.14%, val: 12.07%\n",
      "EPOCH: 188\n",
      "[LOSS] train: 9.516, val: 4.678\n",
      "[ACC%] train: 89.18%, val: 9.2%\n",
      "EPOCH: 189\n",
      "[LOSS] train: 9.485, val: 4.652\n",
      "[ACC%] train: 88.31%, val: 11.49%\n",
      "EPOCH: 190\n",
      "[LOSS] train: 9.496, val: 4.69\n",
      "[ACC%] train: 88.89%, val: 8.05%\n",
      "EPOCH: 191\n",
      "[LOSS] train: 9.518, val: 4.69\n",
      "[ACC%] train: 88.89%, val: 8.05%\n",
      "EPOCH: 192\n",
      "[LOSS] train: 9.529, val: 4.631\n",
      "[ACC%] train: 88.31%, val: 10.92%\n",
      "EPOCH: 193\n",
      "[LOSS] train: 9.47, val: 4.651\n",
      "[ACC%] train: 88.31%, val: 10.34%\n",
      "EPOCH: 194\n",
      "[LOSS] train: 9.412, val: 4.634\n",
      "[ACC%] train: 91.05%, val: 12.64%\n",
      "EPOCH: 195\n",
      "[LOSS] train: 9.373, val: 4.671\n",
      "[ACC%] train: 91.05%, val: 9.77%\n",
      "EPOCH: 196\n",
      "[LOSS] train: 9.37, val: 4.676\n",
      "[ACC%] train: 90.33%, val: 9.2%\n",
      "EPOCH: 197\n",
      "[LOSS] train: 9.379, val: 4.664\n",
      "[ACC%] train: 90.19%, val: 8.05%\n",
      "EPOCH: 198\n",
      "[LOSS] train: 9.491, val: 4.715\n",
      "[ACC%] train: 88.74%, val: 6.9%\n",
      "EPOCH: 199\n",
      "[LOSS] train: 9.404, val: 4.693\n",
      "[ACC%] train: 90.04%, val: 6.9%\n",
      "EPOCH: 200\n",
      "[LOSS] train: 9.422, val: 4.693\n",
      "[ACC%] train: 89.9%, val: 6.9%\n",
      "EPOCH: 201\n",
      "[LOSS] train: 9.299, val: 4.688\n",
      "[ACC%] train: 91.77%, val: 9.2%\n",
      "EPOCH: 202\n",
      "[LOSS] train: 9.256, val: 4.631\n",
      "[ACC%] train: 93.07%, val: 8.62%\n",
      "EPOCH: 203\n",
      "[LOSS] train: 9.337, val: 4.625\n",
      "[ACC%] train: 92.35%, val: 9.2%\n",
      "EPOCH: 204\n",
      "[LOSS] train: 9.239, val: 4.619\n",
      "[ACC%] train: 93.07%, val: 10.34%\n",
      "EPOCH: 205\n",
      "[LOSS] train: 9.275, val: 4.662\n",
      "[ACC%] train: 93.22%, val: 9.77%\n",
      "EPOCH: 206\n",
      "[LOSS] train: 9.244, val: 4.706\n",
      "[ACC%] train: 92.5%, val: 9.77%\n",
      "EPOCH: 207\n",
      "[LOSS] train: 9.333, val: 4.678\n",
      "[ACC%] train: 91.92%, val: 9.77%\n",
      "EPOCH: 208\n",
      "[LOSS] train: 9.287, val: 4.686\n",
      "[ACC%] train: 92.21%, val: 8.05%\n",
      "EPOCH: 209\n",
      "[LOSS] train: 9.325, val: 4.688\n",
      "[ACC%] train: 91.77%, val: 7.47%\n",
      "EPOCH: 210\n",
      "[LOSS] train: 9.299, val: 4.709\n",
      "[ACC%] train: 91.05%, val: 8.62%\n",
      "EPOCH: 211\n",
      "[LOSS] train: 9.418, val: 4.71\n",
      "[ACC%] train: 90.19%, val: 9.2%\n",
      "EPOCH: 212\n",
      "[LOSS] train: 9.31, val: 4.701\n",
      "[ACC%] train: 91.34%, val: 8.62%\n",
      "EPOCH: 213\n",
      "[LOSS] train: 9.286, val: 4.706\n",
      "[ACC%] train: 92.5%, val: 6.9%\n",
      "EPOCH: 214\n",
      "[LOSS] train: 9.344, val: 4.657\n",
      "[ACC%] train: 91.05%, val: 10.34%\n",
      "EPOCH: 215\n",
      "[LOSS] train: 9.306, val: 4.66\n",
      "[ACC%] train: 92.35%, val: 10.34%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 216\n",
      "[LOSS] train: 9.353, val: 4.671\n",
      "[ACC%] train: 91.34%, val: 9.77%\n",
      "EPOCH: 217\n",
      "[LOSS] train: 9.4, val: 4.739\n",
      "[ACC%] train: 89.9%, val: 6.9%\n",
      "EPOCH: 218\n",
      "[LOSS] train: 9.308, val: 4.669\n",
      "[ACC%] train: 92.21%, val: 8.62%\n",
      "EPOCH: 219\n",
      "[LOSS] train: 9.283, val: 4.714\n",
      "[ACC%] train: 92.06%, val: 6.9%\n",
      "EPOCH: 220\n",
      "[LOSS] train: 9.18, val: 4.68\n",
      "[ACC%] train: 93.51%, val: 9.2%\n",
      "EPOCH: 221\n",
      "[LOSS] train: 9.299, val: 4.693\n",
      "[ACC%] train: 91.77%, val: 10.92%\n",
      "EPOCH: 222\n",
      "[LOSS] train: 9.281, val: 4.719\n",
      "[ACC%] train: 92.35%, val: 6.32%\n",
      "EPOCH: 223\n",
      "[LOSS] train: 9.211, val: 4.636\n",
      "[ACC%] train: 92.64%, val: 10.92%\n",
      "EPOCH: 224\n",
      "[LOSS] train: 9.213, val: 4.654\n",
      "[ACC%] train: 92.35%, val: 10.34%\n",
      "EPOCH: 225\n",
      "[LOSS] train: 9.186, val: 4.631\n",
      "[ACC%] train: 93.65%, val: 10.34%\n",
      "EPOCH: 226\n",
      "[LOSS] train: 9.222, val: 4.654\n",
      "[ACC%] train: 92.93%, val: 9.77%\n",
      "EPOCH: 227\n",
      "[LOSS] train: 9.224, val: 4.639\n",
      "[ACC%] train: 92.78%, val: 12.64%\n",
      "EPOCH: 228\n",
      "[LOSS] train: 9.249, val: 4.658\n",
      "[ACC%] train: 93.51%, val: 10.34%\n",
      "EPOCH: 229\n",
      "[LOSS] train: 9.267, val: 4.627\n",
      "[ACC%] train: 92.78%, val: 11.49%\n",
      "EPOCH: 230\n",
      "[LOSS] train: 9.167, val: 4.616\n",
      "[ACC%] train: 94.23%, val: 12.07%\n",
      "EPOCH: 231\n",
      "[LOSS] train: 9.381, val: 4.622\n",
      "[ACC%] train: 90.33%, val: 12.07%\n",
      "EPOCH: 232\n",
      "[LOSS] train: 9.145, val: 4.663\n",
      "[ACC%] train: 93.8%, val: 11.49%\n",
      "EPOCH: 233\n",
      "[LOSS] train: 9.171, val: 4.689\n",
      "[ACC%] train: 93.65%, val: 10.34%\n",
      "EPOCH: 234\n",
      "[LOSS] train: 9.203, val: 4.654\n",
      "[ACC%] train: 92.93%, val: 12.64%\n",
      "EPOCH: 235\n",
      "[LOSS] train: 9.179, val: 4.644\n",
      "[ACC%] train: 94.37%, val: 10.92%\n",
      "EPOCH: 236\n",
      "[LOSS] train: 9.217, val: 4.646\n",
      "[ACC%] train: 92.78%, val: 12.07%\n",
      "EPOCH: 237\n",
      "[LOSS] train: 9.176, val: 4.585\n",
      "[ACC%] train: 93.36%, val: 13.79%\n",
      "EPOCH: 238\n",
      "[LOSS] train: 9.19, val: 4.689\n",
      "[ACC%] train: 93.22%, val: 8.62%\n",
      "EPOCH: 239\n",
      "[LOSS] train: 9.233, val: 4.649\n",
      "[ACC%] train: 92.93%, val: 12.64%\n",
      "EPOCH: 240\n",
      "[LOSS] train: 9.234, val: 4.652\n",
      "[ACC%] train: 92.78%, val: 12.07%\n",
      "EPOCH: 241\n",
      "[LOSS] train: 9.255, val: 4.644\n",
      "[ACC%] train: 93.07%, val: 9.77%\n",
      "EPOCH: 242\n",
      "[LOSS] train: 9.232, val: 4.677\n",
      "[ACC%] train: 92.78%, val: 10.34%\n",
      "EPOCH: 243\n",
      "[LOSS] train: 9.245, val: 4.719\n",
      "[ACC%] train: 93.22%, val: 9.2%\n",
      "EPOCH: 244\n",
      "[LOSS] train: 9.159, val: 4.699\n",
      "[ACC%] train: 94.23%, val: 9.77%\n",
      "EPOCH: 245\n",
      "[LOSS] train: 9.259, val: 4.66\n",
      "[ACC%] train: 93.07%, val: 10.92%\n",
      "EPOCH: 246\n",
      "[LOSS] train: 9.336, val: 4.695\n",
      "[ACC%] train: 90.91%, val: 9.77%\n",
      "EPOCH: 247\n",
      "[LOSS] train: 9.272, val: 4.716\n",
      "[ACC%] train: 91.63%, val: 10.34%\n",
      "EPOCH: 248\n",
      "[LOSS] train: 9.148, val: 4.702\n",
      "[ACC%] train: 93.94%, val: 8.62%\n",
      "EPOCH: 249\n",
      "[LOSS] train: 9.287, val: 4.703\n",
      "[ACC%] train: 92.78%, val: 8.62%\n",
      "EPOCH: 250\n",
      "[LOSS] train: 9.19, val: 4.678\n",
      "[ACC%] train: 92.06%, val: 9.2%\n",
      "EPOCH: 251\n",
      "[LOSS] train: 9.136, val: 4.723\n",
      "[ACC%] train: 94.37%, val: 6.9%\n",
      "EPOCH: 252\n",
      "[LOSS] train: 9.289, val: 4.73\n",
      "[ACC%] train: 92.35%, val: 9.2%\n",
      "EPOCH: 253\n",
      "[LOSS] train: 9.21, val: 4.709\n",
      "[ACC%] train: 93.8%, val: 10.92%\n",
      "EPOCH: 254\n",
      "[LOSS] train: 9.197, val: 4.707\n",
      "[ACC%] train: 93.22%, val: 7.47%\n",
      "EPOCH: 255\n",
      "[LOSS] train: 9.25, val: 4.675\n",
      "[ACC%] train: 92.64%, val: 8.62%\n",
      "EPOCH: 256\n",
      "[LOSS] train: 9.193, val: 4.725\n",
      "[ACC%] train: 93.65%, val: 6.9%\n",
      "EPOCH: 257\n",
      "[LOSS] train: 9.185, val: 4.612\n",
      "[ACC%] train: 93.65%, val: 13.22%\n",
      "EPOCH: 258\n",
      "[LOSS] train: 9.171, val: 4.6\n",
      "[ACC%] train: 93.51%, val: 13.79%\n",
      "EPOCH: 259\n",
      "[LOSS] train: 9.235, val: 4.613\n",
      "[ACC%] train: 92.5%, val: 10.34%\n",
      "EPOCH: 260\n",
      "[LOSS] train: 9.159, val: 4.671\n",
      "[ACC%] train: 93.36%, val: 11.49%\n",
      "EPOCH: 261\n",
      "[LOSS] train: 9.099, val: 4.643\n",
      "[ACC%] train: 95.09%, val: 12.64%\n",
      "EPOCH: 262\n",
      "[LOSS] train: 9.093, val: 4.657\n",
      "[ACC%] train: 95.24%, val: 12.64%\n",
      "EPOCH: 263\n",
      "[LOSS] train: 9.076, val: 4.686\n",
      "[ACC%] train: 95.38%, val: 10.92%\n",
      "EPOCH: 264\n",
      "[LOSS] train: 9.152, val: 4.698\n",
      "[ACC%] train: 93.94%, val: 10.34%\n",
      "EPOCH: 265\n",
      "[LOSS] train: 9.091, val: 4.675\n",
      "[ACC%] train: 94.95%, val: 12.07%\n",
      "EPOCH: 266\n",
      "[LOSS] train: 9.156, val: 4.648\n",
      "[ACC%] train: 93.65%, val: 11.49%\n",
      "EPOCH: 267\n",
      "[LOSS] train: 9.109, val: 4.646\n",
      "[ACC%] train: 94.23%, val: 8.05%\n",
      "EPOCH: 268\n",
      "[LOSS] train: 9.183, val: 4.71\n",
      "[ACC%] train: 93.07%, val: 8.62%\n",
      "EPOCH: 269\n",
      "[LOSS] train: 9.144, val: 4.639\n",
      "[ACC%] train: 94.81%, val: 13.22%\n",
      "EPOCH: 270\n",
      "[LOSS] train: 9.296, val: 4.643\n",
      "[ACC%] train: 92.21%, val: 12.64%\n",
      "EPOCH: 271\n",
      "[LOSS] train: 9.159, val: 4.698\n",
      "[ACC%] train: 93.94%, val: 10.34%\n",
      "EPOCH: 272\n",
      "[LOSS] train: 9.209, val: 4.656\n",
      "[ACC%] train: 92.78%, val: 10.34%\n",
      "EPOCH: 273\n",
      "[LOSS] train: 9.225, val: 4.632\n",
      "[ACC%] train: 93.07%, val: 10.92%\n",
      "EPOCH: 274\n",
      "[LOSS] train: 9.168, val: 4.699\n",
      "[ACC%] train: 93.51%, val: 9.2%\n",
      "EPOCH: 275\n",
      "[LOSS] train: 9.125, val: 4.668\n",
      "[ACC%] train: 94.08%, val: 9.2%\n",
      "EPOCH: 276\n",
      "[LOSS] train: 9.295, val: 4.669\n",
      "[ACC%] train: 91.34%, val: 9.2%\n",
      "EPOCH: 277\n",
      "[LOSS] train: 9.163, val: 4.643\n",
      "[ACC%] train: 94.08%, val: 12.64%\n",
      "EPOCH: 278\n",
      "[LOSS] train: 9.16, val: 4.658\n",
      "[ACC%] train: 93.65%, val: 10.34%\n",
      "EPOCH: 279\n",
      "[LOSS] train: 9.166, val: 4.72\n",
      "[ACC%] train: 93.94%, val: 8.62%\n",
      "EPOCH: 280\n",
      "[LOSS] train: 9.189, val: 4.675\n",
      "[ACC%] train: 93.94%, val: 10.34%\n",
      "EPOCH: 281\n",
      "[LOSS] train: 9.155, val: 4.726\n",
      "[ACC%] train: 93.22%, val: 8.05%\n",
      "EPOCH: 282\n",
      "[LOSS] train: 9.201, val: 4.629\n",
      "[ACC%] train: 92.64%, val: 13.79%\n",
      "EPOCH: 283\n",
      "[LOSS] train: 9.153, val: 4.652\n",
      "[ACC%] train: 94.66%, val: 11.49%\n",
      "EPOCH: 284\n",
      "[LOSS] train: 9.174, val: 4.654\n",
      "[ACC%] train: 93.07%, val: 11.49%\n",
      "EPOCH: 285\n",
      "[LOSS] train: 9.236, val: 4.703\n",
      "[ACC%] train: 92.5%, val: 8.62%\n",
      "EPOCH: 286\n",
      "[LOSS] train: 9.237, val: 4.647\n",
      "[ACC%] train: 93.36%, val: 12.64%\n",
      "EPOCH: 287\n",
      "[LOSS] train: 9.202, val: 4.614\n",
      "[ACC%] train: 92.78%, val: 11.49%\n",
      "EPOCH: 288\n",
      "[LOSS] train: 9.208, val: 4.669\n",
      "[ACC%] train: 93.51%, val: 11.49%\n",
      "EPOCH: 289\n",
      "[LOSS] train: 9.172, val: 4.687\n",
      "[ACC%] train: 94.23%, val: 8.62%\n",
      "EPOCH: 290\n",
      "[LOSS] train: 9.216, val: 4.633\n",
      "[ACC%] train: 93.07%, val: 8.62%\n",
      "EPOCH: 291\n",
      "[LOSS] train: 9.312, val: 4.659\n",
      "[ACC%] train: 91.49%, val: 9.77%\n",
      "EPOCH: 292\n",
      "[LOSS] train: 9.161, val: 4.67\n",
      "[ACC%] train: 93.65%, val: 13.22%\n",
      "EPOCH: 293\n",
      "[LOSS] train: 9.135, val: 4.66\n",
      "[ACC%] train: 93.94%, val: 9.2%\n",
      "EPOCH: 294\n",
      "[LOSS] train: 9.198, val: 4.731\n",
      "[ACC%] train: 93.22%, val: 8.05%\n",
      "EPOCH: 295\n",
      "[LOSS] train: 9.203, val: 4.706\n",
      "[ACC%] train: 93.22%, val: 9.77%\n",
      "EPOCH: 296\n",
      "[LOSS] train: 9.095, val: 4.637\n",
      "[ACC%] train: 94.81%, val: 9.2%\n",
      "EPOCH: 297\n",
      "[LOSS] train: 9.144, val: 4.715\n",
      "[ACC%] train: 94.08%, val: 9.77%\n",
      "EPOCH: 298\n",
      "[LOSS] train: 9.109, val: 4.735\n",
      "[ACC%] train: 94.37%, val: 7.47%\n",
      "EPOCH: 299\n",
      "[LOSS] train: 9.185, val: 4.699\n",
      "[ACC%] train: 93.65%, val: 10.34%\n",
      "EPOCH: 300\n",
      "[LOSS] train: 9.234, val: 4.664\n",
      "[ACC%] train: 92.93%, val: 12.07%\n",
      "EPOCH: 301\n",
      "[LOSS] train: 9.161, val: 4.714\n",
      "[ACC%] train: 94.23%, val: 9.2%\n",
      "EPOCH: 302\n",
      "[LOSS] train: 9.121, val: 4.731\n",
      "[ACC%] train: 93.94%, val: 8.05%\n",
      "EPOCH: 303\n",
      "[LOSS] train: 9.085, val: 4.707\n",
      "[ACC%] train: 94.81%, val: 8.62%\n",
      "EPOCH: 304\n",
      "[LOSS] train: 9.114, val: 4.7\n",
      "[ACC%] train: 94.37%, val: 6.9%\n",
      "EPOCH: 305\n",
      "[LOSS] train: 9.083, val: 4.756\n",
      "[ACC%] train: 94.95%, val: 6.9%\n",
      "EPOCH: 306\n",
      "[LOSS] train: 9.093, val: 4.737\n",
      "[ACC%] train: 94.08%, val: 9.2%\n",
      "EPOCH: 307\n",
      "[LOSS] train: 9.116, val: 4.655\n",
      "[ACC%] train: 94.37%, val: 9.77%\n",
      "EPOCH: 308\n",
      "[LOSS] train: 9.067, val: 4.647\n",
      "[ACC%] train: 95.24%, val: 8.05%\n",
      "EPOCH: 309\n",
      "[LOSS] train: 9.129, val: 4.658\n",
      "[ACC%] train: 93.36%, val: 10.92%\n",
      "EPOCH: 310\n",
      "[LOSS] train: 9.07, val: 4.675\n",
      "[ACC%] train: 95.24%, val: 9.2%\n",
      "EPOCH: 311\n",
      "[LOSS] train: 9.172, val: 4.704\n",
      "[ACC%] train: 93.51%, val: 8.05%\n",
      "EPOCH: 312\n",
      "[LOSS] train: 9.064, val: 4.728\n",
      "[ACC%] train: 95.24%, val: 9.2%\n",
      "EPOCH: 313\n",
      "[LOSS] train: 9.022, val: 4.653\n",
      "[ACC%] train: 96.39%, val: 8.05%\n",
      "EPOCH: 314\n",
      "[LOSS] train: 9.063, val: 4.704\n",
      "[ACC%] train: 95.24%, val: 9.77%\n",
      "EPOCH: 315\n",
      "[LOSS] train: 9.029, val: 4.672\n",
      "[ACC%] train: 95.53%, val: 10.92%\n",
      "EPOCH: 316\n",
      "[LOSS] train: 9.074, val: 4.626\n",
      "[ACC%] train: 94.81%, val: 12.07%\n",
      "EPOCH: 317\n",
      "[LOSS] train: 9.075, val: 4.687\n",
      "[ACC%] train: 95.38%, val: 12.64%\n",
      "EPOCH: 318\n",
      "[LOSS] train: 9.039, val: 4.695\n",
      "[ACC%] train: 95.82%, val: 10.92%\n",
      "EPOCH: 319\n",
      "[LOSS] train: 9.037, val: 4.699\n",
      "[ACC%] train: 95.09%, val: 9.2%\n",
      "EPOCH: 320\n",
      "[LOSS] train: 9.016, val: 4.733\n",
      "[ACC%] train: 96.39%, val: 8.05%\n",
      "EPOCH: 321\n",
      "[LOSS] train: 9.076, val: 4.697\n",
      "[ACC%] train: 94.66%, val: 9.2%\n",
      "EPOCH: 322\n",
      "[LOSS] train: 9.047, val: 4.766\n",
      "[ACC%] train: 95.24%, val: 7.47%\n",
      "EPOCH: 323\n",
      "[LOSS] train: 9.081, val: 4.698\n",
      "[ACC%] train: 94.66%, val: 10.34%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 324\n",
      "[LOSS] train: 9.067, val: 4.722\n",
      "[ACC%] train: 95.09%, val: 9.77%\n",
      "EPOCH: 325\n",
      "[LOSS] train: 8.991, val: 4.7\n",
      "[ACC%] train: 96.39%, val: 8.62%\n",
      "EPOCH: 326\n",
      "[LOSS] train: 9.033, val: 4.67\n",
      "[ACC%] train: 95.67%, val: 8.62%\n",
      "EPOCH: 327\n",
      "[LOSS] train: 9.05, val: 4.701\n",
      "[ACC%] train: 94.95%, val: 9.77%\n",
      "EPOCH: 328\n",
      "[LOSS] train: 9.06, val: 4.726\n",
      "[ACC%] train: 95.38%, val: 7.47%\n",
      "EPOCH: 329\n",
      "[LOSS] train: 9.149, val: 4.658\n",
      "[ACC%] train: 94.37%, val: 9.2%\n",
      "EPOCH: 330\n",
      "[LOSS] train: 9.033, val: 4.654\n",
      "[ACC%] train: 95.82%, val: 10.92%\n",
      "EPOCH: 331\n",
      "[LOSS] train: 9.004, val: 4.674\n",
      "[ACC%] train: 96.54%, val: 10.34%\n",
      "EPOCH: 332\n",
      "[LOSS] train: 9.049, val: 4.656\n",
      "[ACC%] train: 95.53%, val: 8.62%\n",
      "EPOCH: 333\n",
      "[LOSS] train: 9.047, val: 4.733\n",
      "[ACC%] train: 95.38%, val: 6.9%\n",
      "EPOCH: 334\n",
      "[LOSS] train: 9.041, val: 4.7\n",
      "[ACC%] train: 95.96%, val: 9.77%\n",
      "EPOCH: 335\n",
      "[LOSS] train: 9.05, val: 4.684\n",
      "[ACC%] train: 96.54%, val: 10.34%\n",
      "EPOCH: 336\n",
      "[LOSS] train: 9.034, val: 4.752\n",
      "[ACC%] train: 95.09%, val: 6.9%\n",
      "EPOCH: 337\n",
      "[LOSS] train: 9.034, val: 4.757\n",
      "[ACC%] train: 95.67%, val: 8.05%\n",
      "EPOCH: 338\n",
      "[LOSS] train: 9.112, val: 4.719\n",
      "[ACC%] train: 94.66%, val: 10.92%\n",
      "EPOCH: 339\n",
      "[LOSS] train: 9.067, val: 4.702\n",
      "[ACC%] train: 95.24%, val: 9.77%\n",
      "EPOCH: 340\n",
      "[LOSS] train: 9.078, val: 4.694\n",
      "[ACC%] train: 94.66%, val: 8.62%\n",
      "EPOCH: 341\n",
      "[LOSS] train: 9.04, val: 4.71\n",
      "[ACC%] train: 95.53%, val: 8.62%\n",
      "EPOCH: 342\n",
      "[LOSS] train: 9.014, val: 4.712\n",
      "[ACC%] train: 96.1%, val: 8.62%\n",
      "EPOCH: 343\n",
      "[LOSS] train: 9.089, val: 4.698\n",
      "[ACC%] train: 95.09%, val: 10.92%\n",
      "EPOCH: 344\n",
      "[LOSS] train: 9.069, val: 4.698\n",
      "[ACC%] train: 94.95%, val: 9.77%\n",
      "EPOCH: 345\n",
      "[LOSS] train: 8.977, val: 4.707\n",
      "[ACC%] train: 96.83%, val: 10.92%\n",
      "EPOCH: 346\n",
      "[LOSS] train: 9.002, val: 4.684\n",
      "[ACC%] train: 96.39%, val: 9.77%\n",
      "EPOCH: 347\n",
      "[LOSS] train: 9.038, val: 4.698\n",
      "[ACC%] train: 95.82%, val: 8.62%\n",
      "EPOCH: 348\n",
      "[LOSS] train: 9.101, val: 4.633\n",
      "[ACC%] train: 94.52%, val: 11.49%\n",
      "EPOCH: 349\n",
      "[LOSS] train: 9.016, val: 4.636\n",
      "[ACC%] train: 95.82%, val: 10.34%\n",
      "EPOCH: 350\n",
      "[LOSS] train: 9.006, val: 4.714\n",
      "[ACC%] train: 96.25%, val: 9.77%\n",
      "EPOCH: 351\n",
      "[LOSS] train: 9.005, val: 4.645\n",
      "[ACC%] train: 96.39%, val: 12.07%\n",
      "EPOCH: 352\n",
      "[LOSS] train: 9.053, val: 4.699\n",
      "[ACC%] train: 95.24%, val: 9.77%\n",
      "EPOCH: 353\n",
      "[LOSS] train: 9.012, val: 4.729\n",
      "[ACC%] train: 96.39%, val: 9.2%\n",
      "EPOCH: 354\n",
      "[LOSS] train: 9.017, val: 4.69\n",
      "[ACC%] train: 96.54%, val: 9.2%\n",
      "EPOCH: 355\n",
      "[LOSS] train: 9.064, val: 4.658\n",
      "[ACC%] train: 95.38%, val: 10.92%\n",
      "EPOCH: 356\n",
      "[LOSS] train: 9.058, val: 4.644\n",
      "[ACC%] train: 95.96%, val: 11.49%\n",
      "EPOCH: 357\n",
      "[LOSS] train: 9.025, val: 4.679\n",
      "[ACC%] train: 96.25%, val: 9.77%\n",
      "EPOCH: 358\n",
      "[LOSS] train: 9.019, val: 4.623\n",
      "[ACC%] train: 95.96%, val: 9.77%\n",
      "EPOCH: 359\n",
      "[LOSS] train: 9.079, val: 4.645\n",
      "[ACC%] train: 95.38%, val: 10.92%\n",
      "EPOCH: 360\n",
      "[LOSS] train: 9.018, val: 4.689\n",
      "[ACC%] train: 95.82%, val: 8.05%\n",
      "EPOCH: 361\n",
      "[LOSS] train: 9.117, val: 4.717\n",
      "[ACC%] train: 95.38%, val: 9.2%\n",
      "EPOCH: 362\n",
      "[LOSS] train: 9.113, val: 4.607\n",
      "[ACC%] train: 94.66%, val: 12.64%\n",
      "EPOCH: 363\n",
      "[LOSS] train: 9.104, val: 4.677\n",
      "[ACC%] train: 94.23%, val: 9.2%\n",
      "EPOCH: 364\n",
      "[LOSS] train: 9.113, val: 4.718\n",
      "[ACC%] train: 94.66%, val: 9.2%\n",
      "EPOCH: 365\n",
      "[LOSS] train: 9.144, val: 4.682\n",
      "[ACC%] train: 94.81%, val: 12.07%\n",
      "EPOCH: 366\n",
      "[LOSS] train: 9.106, val: 4.701\n",
      "[ACC%] train: 94.66%, val: 10.92%\n",
      "EPOCH: 367\n",
      "[LOSS] train: 9.051, val: 4.684\n",
      "[ACC%] train: 95.24%, val: 9.77%\n",
      "EPOCH: 368\n",
      "[LOSS] train: 9.162, val: 4.606\n",
      "[ACC%] train: 93.94%, val: 13.79%\n",
      "EPOCH: 369\n",
      "[LOSS] train: 9.098, val: 4.687\n",
      "[ACC%] train: 94.81%, val: 9.77%\n",
      "EPOCH: 370\n",
      "[LOSS] train: 9.005, val: 4.665\n",
      "[ACC%] train: 96.54%, val: 10.92%\n",
      "EPOCH: 371\n",
      "[LOSS] train: 9.084, val: 4.673\n",
      "[ACC%] train: 95.09%, val: 10.92%\n",
      "EPOCH: 372\n",
      "[LOSS] train: 9.021, val: 4.692\n",
      "[ACC%] train: 95.96%, val: 9.2%\n",
      "EPOCH: 373\n",
      "[LOSS] train: 9.109, val: 4.687\n",
      "[ACC%] train: 94.52%, val: 8.62%\n",
      "EPOCH: 374\n",
      "[LOSS] train: 9.071, val: 4.677\n",
      "[ACC%] train: 95.09%, val: 12.07%\n",
      "EPOCH: 375\n",
      "[LOSS] train: 9.089, val: 4.688\n",
      "[ACC%] train: 94.37%, val: 8.62%\n",
      "EPOCH: 376\n",
      "[LOSS] train: 9.073, val: 4.629\n",
      "[ACC%] train: 95.38%, val: 11.49%\n",
      "EPOCH: 377\n",
      "[LOSS] train: 9.011, val: 4.693\n",
      "[ACC%] train: 95.96%, val: 10.92%\n",
      "EPOCH: 378\n",
      "[LOSS] train: 9.084, val: 4.664\n",
      "[ACC%] train: 95.09%, val: 10.92%\n",
      "EPOCH: 379\n",
      "[LOSS] train: 9.102, val: 4.707\n",
      "[ACC%] train: 94.23%, val: 10.92%\n",
      "EPOCH: 380\n",
      "[LOSS] train: 8.982, val: 4.696\n",
      "[ACC%] train: 96.39%, val: 10.92%\n",
      "EPOCH: 381\n",
      "[LOSS] train: 9.106, val: 4.704\n",
      "[ACC%] train: 94.23%, val: 9.2%\n",
      "EPOCH: 382\n",
      "[LOSS] train: 9.018, val: 4.715\n",
      "[ACC%] train: 95.82%, val: 9.2%\n",
      "EPOCH: 383\n",
      "[LOSS] train: 9.121, val: 4.69\n",
      "[ACC%] train: 95.09%, val: 10.34%\n",
      "EPOCH: 384\n",
      "[LOSS] train: 9.019, val: 4.692\n",
      "[ACC%] train: 95.67%, val: 10.34%\n",
      "EPOCH: 385\n",
      "[LOSS] train: 9.041, val: 4.676\n",
      "[ACC%] train: 96.25%, val: 12.07%\n",
      "EPOCH: 386\n",
      "[LOSS] train: 9.091, val: 4.65\n",
      "[ACC%] train: 95.24%, val: 12.07%\n",
      "EPOCH: 387\n",
      "[LOSS] train: 9.027, val: 4.686\n",
      "[ACC%] train: 95.38%, val: 9.77%\n",
      "EPOCH: 388\n",
      "[LOSS] train: 9.011, val: 4.699\n",
      "[ACC%] train: 96.39%, val: 9.77%\n",
      "EPOCH: 389\n",
      "[LOSS] train: 9.008, val: 4.691\n",
      "[ACC%] train: 96.25%, val: 9.2%\n",
      "EPOCH: 390\n",
      "[LOSS] train: 9.045, val: 4.751\n",
      "[ACC%] train: 95.67%, val: 9.2%\n",
      "EPOCH: 391\n",
      "[LOSS] train: 9.082, val: 4.702\n",
      "[ACC%] train: 94.95%, val: 9.2%\n",
      "EPOCH: 392\n",
      "[LOSS] train: 9.082, val: 4.731\n",
      "[ACC%] train: 95.24%, val: 8.05%\n",
      "EPOCH: 393\n",
      "[LOSS] train: 9.12, val: 4.717\n",
      "[ACC%] train: 94.37%, val: 8.05%\n",
      "EPOCH: 394\n",
      "[LOSS] train: 9.054, val: 4.699\n",
      "[ACC%] train: 94.81%, val: 9.77%\n",
      "EPOCH: 395\n",
      "[LOSS] train: 9.141, val: 4.643\n",
      "[ACC%] train: 93.65%, val: 10.92%\n",
      "EPOCH: 396\n",
      "[LOSS] train: 9.132, val: 4.677\n",
      "[ACC%] train: 94.52%, val: 10.34%\n",
      "EPOCH: 397\n",
      "[LOSS] train: 9.078, val: 4.641\n",
      "[ACC%] train: 95.53%, val: 9.77%\n",
      "EPOCH: 398\n",
      "[LOSS] train: 9.035, val: 4.713\n",
      "[ACC%] train: 95.24%, val: 8.05%\n",
      "EPOCH: 399\n",
      "[LOSS] train: 9.022, val: 4.726\n",
      "[ACC%] train: 95.82%, val: 5.75%\n",
      "EPOCH: 400\n",
      "[LOSS] train: 8.995, val: 4.712\n",
      "[ACC%] train: 96.25%, val: 8.05%\n",
      "EPOCH: 401\n",
      "[LOSS] train: 9.011, val: 4.697\n",
      "[ACC%] train: 95.96%, val: 9.77%\n",
      "EPOCH: 402\n",
      "[LOSS] train: 9.105, val: 4.713\n",
      "[ACC%] train: 94.66%, val: 9.2%\n",
      "EPOCH: 403\n",
      "[LOSS] train: 8.982, val: 4.654\n",
      "[ACC%] train: 96.68%, val: 10.92%\n",
      "EPOCH: 404\n",
      "[LOSS] train: 9.001, val: 4.706\n",
      "[ACC%] train: 96.39%, val: 9.77%\n",
      "EPOCH: 405\n",
      "[LOSS] train: 9.104, val: 4.722\n",
      "[ACC%] train: 94.95%, val: 8.62%\n",
      "EPOCH: 406\n",
      "[LOSS] train: 9.07, val: 4.639\n",
      "[ACC%] train: 94.81%, val: 12.07%\n",
      "EPOCH: 407\n",
      "[LOSS] train: 9.103, val: 4.694\n",
      "[ACC%] train: 94.37%, val: 9.2%\n",
      "EPOCH: 408\n",
      "[LOSS] train: 9.011, val: 4.654\n",
      "[ACC%] train: 95.67%, val: 10.92%\n",
      "EPOCH: 409\n",
      "[LOSS] train: 9.11, val: 4.698\n",
      "[ACC%] train: 94.37%, val: 13.22%\n",
      "EPOCH: 410\n",
      "[LOSS] train: 9.053, val: 4.686\n",
      "[ACC%] train: 95.67%, val: 11.49%\n",
      "EPOCH: 411\n",
      "[LOSS] train: 8.956, val: 4.704\n",
      "[ACC%] train: 97.11%, val: 10.92%\n",
      "EPOCH: 412\n",
      "[LOSS] train: 9.029, val: 4.692\n",
      "[ACC%] train: 95.53%, val: 9.2%\n",
      "EPOCH: 413\n",
      "[LOSS] train: 9.068, val: 4.716\n",
      "[ACC%] train: 95.38%, val: 6.9%\n",
      "EPOCH: 414\n",
      "[LOSS] train: 8.95, val: 4.679\n",
      "[ACC%] train: 96.68%, val: 12.07%\n",
      "EPOCH: 415\n",
      "[LOSS] train: 8.998, val: 4.72\n",
      "[ACC%] train: 95.82%, val: 7.47%\n",
      "EPOCH: 416\n",
      "[LOSS] train: 9.029, val: 4.74\n",
      "[ACC%] train: 96.1%, val: 9.2%\n",
      "EPOCH: 417\n",
      "[LOSS] train: 9.002, val: 4.721\n",
      "[ACC%] train: 96.1%, val: 8.05%\n",
      "EPOCH: 418\n",
      "[LOSS] train: 9.042, val: 4.722\n",
      "[ACC%] train: 95.53%, val: 10.92%\n",
      "EPOCH: 419\n",
      "[LOSS] train: 8.961, val: 4.694\n",
      "[ACC%] train: 97.11%, val: 9.77%\n",
      "EPOCH: 420\n",
      "[LOSS] train: 9.017, val: 4.629\n",
      "[ACC%] train: 96.1%, val: 11.49%\n",
      "EPOCH: 421\n",
      "[LOSS] train: 9.014, val: 4.623\n",
      "[ACC%] train: 95.96%, val: 14.37%\n",
      "EPOCH: 422\n",
      "[LOSS] train: 9.042, val: 4.597\n",
      "[ACC%] train: 95.67%, val: 13.79%\n",
      "EPOCH: 423\n",
      "[LOSS] train: 9.14, val: 4.664\n",
      "[ACC%] train: 94.23%, val: 11.49%\n",
      "EPOCH: 424\n",
      "[LOSS] train: 9.111, val: 4.673\n",
      "[ACC%] train: 94.66%, val: 10.92%\n",
      "EPOCH: 425\n",
      "[LOSS] train: 9.104, val: 4.694\n",
      "[ACC%] train: 95.38%, val: 12.64%\n",
      "EPOCH: 426\n",
      "[LOSS] train: 9.241, val: 4.681\n",
      "[ACC%] train: 91.92%, val: 13.79%\n",
      "EPOCH: 427\n",
      "[LOSS] train: 9.036, val: 4.734\n",
      "[ACC%] train: 96.54%, val: 8.62%\n",
      "EPOCH: 428\n",
      "[LOSS] train: 9.009, val: 4.7\n",
      "[ACC%] train: 95.67%, val: 8.62%\n",
      "EPOCH: 429\n",
      "[LOSS] train: 9.029, val: 4.667\n",
      "[ACC%] train: 95.24%, val: 10.92%\n",
      "EPOCH: 430\n",
      "[LOSS] train: 9.059, val: 4.698\n",
      "[ACC%] train: 94.81%, val: 10.34%\n",
      "EPOCH: 431\n",
      "[LOSS] train: 9.012, val: 4.683\n",
      "[ACC%] train: 96.25%, val: 11.49%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 432\n",
      "[LOSS] train: 8.991, val: 4.658\n",
      "[ACC%] train: 95.82%, val: 11.49%\n",
      "EPOCH: 433\n",
      "[LOSS] train: 9.054, val: 4.663\n",
      "[ACC%] train: 95.24%, val: 12.64%\n",
      "EPOCH: 434\n",
      "[LOSS] train: 9.071, val: 4.679\n",
      "[ACC%] train: 94.95%, val: 10.34%\n",
      "EPOCH: 435\n",
      "[LOSS] train: 9.02, val: 4.601\n",
      "[ACC%] train: 95.96%, val: 13.79%\n",
      "EPOCH: 436\n",
      "[LOSS] train: 9.071, val: 4.621\n",
      "[ACC%] train: 95.24%, val: 13.22%\n",
      "EPOCH: 437\n",
      "[LOSS] train: 9.007, val: 4.608\n",
      "[ACC%] train: 96.1%, val: 13.22%\n",
      "EPOCH: 438\n",
      "[LOSS] train: 9.091, val: 4.705\n",
      "[ACC%] train: 95.38%, val: 9.2%\n",
      "EPOCH: 439\n",
      "[LOSS] train: 9.057, val: 4.694\n",
      "[ACC%] train: 95.09%, val: 9.77%\n",
      "EPOCH: 440\n",
      "[LOSS] train: 9.042, val: 4.65\n",
      "[ACC%] train: 95.24%, val: 11.49%\n",
      "EPOCH: 441\n",
      "[LOSS] train: 9.042, val: 4.617\n",
      "[ACC%] train: 95.53%, val: 14.37%\n",
      "EPOCH: 442\n",
      "[LOSS] train: 9.056, val: 4.695\n",
      "[ACC%] train: 95.53%, val: 11.49%\n",
      "EPOCH: 443\n",
      "[LOSS] train: 9.105, val: 4.642\n",
      "[ACC%] train: 94.95%, val: 12.07%\n",
      "EPOCH: 444\n",
      "[LOSS] train: 8.952, val: 4.68\n",
      "[ACC%] train: 97.11%, val: 10.34%\n",
      "EPOCH: 445\n",
      "[LOSS] train: 9.03, val: 4.719\n",
      "[ACC%] train: 95.96%, val: 10.92%\n",
      "EPOCH: 446\n",
      "[LOSS] train: 9.052, val: 4.686\n",
      "[ACC%] train: 95.38%, val: 10.92%\n",
      "EPOCH: 447\n",
      "[LOSS] train: 9.045, val: 4.654\n",
      "[ACC%] train: 95.38%, val: 11.49%\n",
      "EPOCH: 448\n",
      "[LOSS] train: 9.072, val: 4.699\n",
      "[ACC%] train: 95.38%, val: 11.49%\n",
      "EPOCH: 449\n",
      "[LOSS] train: 9.068, val: 4.674\n",
      "[ACC%] train: 94.52%, val: 11.49%\n",
      "EPOCH: 450\n",
      "[LOSS] train: 9.108, val: 4.679\n",
      "[ACC%] train: 94.66%, val: 12.64%\n",
      "EPOCH: 451\n",
      "[LOSS] train: 9.136, val: 4.633\n",
      "[ACC%] train: 94.52%, val: 11.49%\n",
      "EPOCH: 452\n",
      "[LOSS] train: 9.092, val: 4.703\n",
      "[ACC%] train: 94.95%, val: 9.77%\n",
      "EPOCH: 453\n",
      "[LOSS] train: 9.125, val: 4.691\n",
      "[ACC%] train: 94.81%, val: 9.77%\n",
      "EPOCH: 454\n",
      "[LOSS] train: 9.064, val: 4.695\n",
      "[ACC%] train: 95.09%, val: 10.92%\n",
      "EPOCH: 455\n",
      "[LOSS] train: 9.223, val: 4.602\n",
      "[ACC%] train: 93.36%, val: 13.22%\n",
      "EPOCH: 456\n",
      "[LOSS] train: 9.119, val: 4.719\n",
      "[ACC%] train: 94.66%, val: 10.34%\n",
      "EPOCH: 457\n",
      "[LOSS] train: 9.111, val: 4.681\n",
      "[ACC%] train: 94.52%, val: 9.77%\n",
      "EPOCH: 458\n",
      "[LOSS] train: 9.129, val: 4.69\n",
      "[ACC%] train: 94.66%, val: 9.77%\n",
      "EPOCH: 459\n",
      "[LOSS] train: 9.124, val: 4.707\n",
      "[ACC%] train: 94.95%, val: 8.62%\n",
      "EPOCH: 460\n",
      "[LOSS] train: 9.04, val: 4.662\n",
      "[ACC%] train: 95.09%, val: 11.49%\n",
      "EPOCH: 461\n",
      "[LOSS] train: 9.091, val: 4.675\n",
      "[ACC%] train: 94.52%, val: 10.92%\n",
      "EPOCH: 462\n",
      "[LOSS] train: 9.024, val: 4.706\n",
      "[ACC%] train: 95.82%, val: 10.34%\n",
      "EPOCH: 463\n",
      "[LOSS] train: 9.102, val: 4.681\n",
      "[ACC%] train: 94.37%, val: 10.92%\n",
      "EPOCH: 464\n",
      "[LOSS] train: 9.189, val: 4.686\n",
      "[ACC%] train: 93.94%, val: 9.77%\n",
      "EPOCH: 465\n",
      "[LOSS] train: 9.242, val: 4.638\n",
      "[ACC%] train: 92.64%, val: 11.49%\n",
      "EPOCH: 466\n",
      "[LOSS] train: 9.104, val: 4.678\n",
      "[ACC%] train: 94.08%, val: 9.77%\n",
      "EPOCH: 467\n",
      "[LOSS] train: 9.089, val: 4.691\n",
      "[ACC%] train: 94.81%, val: 10.92%\n",
      "EPOCH: 468\n",
      "[LOSS] train: 9.077, val: 4.7\n",
      "[ACC%] train: 95.38%, val: 9.77%\n",
      "EPOCH: 469\n",
      "[LOSS] train: 9.074, val: 4.666\n",
      "[ACC%] train: 94.66%, val: 8.05%\n",
      "EPOCH: 470\n",
      "[LOSS] train: 9.06, val: 4.727\n",
      "[ACC%] train: 95.67%, val: 9.2%\n",
      "EPOCH: 471\n",
      "[LOSS] train: 9.112, val: 4.667\n",
      "[ACC%] train: 94.52%, val: 12.64%\n",
      "EPOCH: 472\n",
      "[LOSS] train: 9.075, val: 4.684\n",
      "[ACC%] train: 94.95%, val: 12.07%\n",
      "EPOCH: 473\n",
      "[LOSS] train: 9.057, val: 4.664\n",
      "[ACC%] train: 95.53%, val: 10.34%\n",
      "EPOCH: 474\n",
      "[LOSS] train: 9.023, val: 4.682\n",
      "[ACC%] train: 95.82%, val: 9.2%\n",
      "EPOCH: 475\n",
      "[LOSS] train: 9.036, val: 4.689\n",
      "[ACC%] train: 95.96%, val: 12.07%\n",
      "EPOCH: 476\n",
      "[LOSS] train: 8.981, val: 4.705\n",
      "[ACC%] train: 96.25%, val: 9.2%\n",
      "EPOCH: 477\n",
      "[LOSS] train: 9.008, val: 4.707\n",
      "[ACC%] train: 95.67%, val: 8.05%\n",
      "EPOCH: 478\n",
      "[LOSS] train: 9.041, val: 4.741\n",
      "[ACC%] train: 95.96%, val: 9.77%\n",
      "EPOCH: 479\n",
      "[LOSS] train: 9.0, val: 4.689\n",
      "[ACC%] train: 96.1%, val: 10.34%\n",
      "EPOCH: 480\n",
      "[LOSS] train: 9.025, val: 4.715\n",
      "[ACC%] train: 95.82%, val: 9.77%\n",
      "EPOCH: 481\n",
      "[LOSS] train: 8.989, val: 4.728\n",
      "[ACC%] train: 96.39%, val: 8.62%\n",
      "EPOCH: 482\n",
      "[LOSS] train: 8.971, val: 4.763\n",
      "[ACC%] train: 96.39%, val: 8.62%\n",
      "EPOCH: 483\n",
      "[LOSS] train: 8.976, val: 4.678\n",
      "[ACC%] train: 96.54%, val: 10.92%\n",
      "EPOCH: 484\n",
      "[LOSS] train: 8.986, val: 4.74\n",
      "[ACC%] train: 96.1%, val: 8.62%\n",
      "EPOCH: 485\n",
      "[LOSS] train: 8.902, val: 4.697\n",
      "[ACC%] train: 97.55%, val: 9.2%\n",
      "EPOCH: 486\n",
      "[LOSS] train: 9.06, val: 4.664\n",
      "[ACC%] train: 95.38%, val: 9.77%\n",
      "EPOCH: 487\n",
      "[LOSS] train: 8.908, val: 4.676\n",
      "[ACC%] train: 97.4%, val: 9.77%\n",
      "EPOCH: 488\n",
      "[LOSS] train: 8.973, val: 4.701\n",
      "[ACC%] train: 96.68%, val: 9.2%\n",
      "EPOCH: 489\n",
      "[LOSS] train: 9.03, val: 4.706\n",
      "[ACC%] train: 95.96%, val: 8.05%\n",
      "EPOCH: 490\n",
      "[LOSS] train: 8.988, val: 4.69\n",
      "[ACC%] train: 96.39%, val: 10.92%\n",
      "EPOCH: 491\n",
      "[LOSS] train: 8.991, val: 4.71\n",
      "[ACC%] train: 96.68%, val: 9.2%\n",
      "EPOCH: 492\n",
      "[LOSS] train: 8.965, val: 4.731\n",
      "[ACC%] train: 96.83%, val: 8.62%\n",
      "EPOCH: 493\n",
      "[LOSS] train: 8.965, val: 4.699\n",
      "[ACC%] train: 96.83%, val: 11.49%\n",
      "EPOCH: 494\n",
      "[LOSS] train: 8.964, val: 4.712\n",
      "[ACC%] train: 96.83%, val: 9.2%\n",
      "EPOCH: 495\n",
      "[LOSS] train: 9.004, val: 4.673\n",
      "[ACC%] train: 96.25%, val: 10.34%\n",
      "EPOCH: 496\n",
      "[LOSS] train: 8.994, val: 4.674\n",
      "[ACC%] train: 95.67%, val: 10.92%\n",
      "EPOCH: 497\n",
      "[LOSS] train: 8.941, val: 4.699\n",
      "[ACC%] train: 97.4%, val: 12.07%\n",
      "EPOCH: 498\n",
      "[LOSS] train: 8.965, val: 4.664\n",
      "[ACC%] train: 96.68%, val: 11.49%\n",
      "EPOCH: 499\n",
      "[LOSS] train: 9.121, val: 4.704\n",
      "[ACC%] train: 94.37%, val: 9.2%\n",
      "EPOCH: 500\n",
      "[LOSS] train: 9.002, val: 4.732\n",
      "[ACC%] train: 96.39%, val: 8.05%\n",
      "EPOCH: 501\n",
      "[LOSS] train: 8.986, val: 4.747\n",
      "[ACC%] train: 96.39%, val: 7.47%\n",
      "EPOCH: 502\n",
      "[LOSS] train: 9.109, val: 4.639\n",
      "[ACC%] train: 94.23%, val: 10.92%\n",
      "EPOCH: 503\n",
      "[LOSS] train: 9.042, val: 4.694\n",
      "[ACC%] train: 96.1%, val: 11.49%\n",
      "EPOCH: 504\n",
      "[LOSS] train: 9.026, val: 4.716\n",
      "[ACC%] train: 96.1%, val: 9.77%\n",
      "EPOCH: 505\n",
      "[LOSS] train: 9.002, val: 4.684\n",
      "[ACC%] train: 95.67%, val: 10.92%\n",
      "EPOCH: 506\n",
      "[LOSS] train: 9.023, val: 4.735\n",
      "[ACC%] train: 95.96%, val: 7.47%\n",
      "EPOCH: 507\n",
      "[LOSS] train: 9.036, val: 4.667\n",
      "[ACC%] train: 95.24%, val: 10.34%\n",
      "EPOCH: 508\n",
      "[LOSS] train: 9.059, val: 4.672\n",
      "[ACC%] train: 95.53%, val: 10.92%\n",
      "EPOCH: 509\n",
      "[LOSS] train: 8.982, val: 4.743\n",
      "[ACC%] train: 96.97%, val: 7.47%\n",
      "EPOCH: 510\n",
      "[LOSS] train: 8.984, val: 4.77\n",
      "[ACC%] train: 96.39%, val: 6.9%\n",
      "EPOCH: 511\n",
      "[LOSS] train: 9.044, val: 4.714\n",
      "[ACC%] train: 95.53%, val: 8.62%\n",
      "EPOCH: 512\n",
      "[LOSS] train: 8.985, val: 4.686\n",
      "[ACC%] train: 96.39%, val: 10.34%\n",
      "EPOCH: 513\n",
      "[LOSS] train: 9.007, val: 4.67\n",
      "[ACC%] train: 96.1%, val: 9.2%\n",
      "EPOCH: 514\n",
      "[LOSS] train: 8.966, val: 4.703\n",
      "[ACC%] train: 97.11%, val: 9.2%\n",
      "EPOCH: 515\n",
      "[LOSS] train: 9.008, val: 4.702\n",
      "[ACC%] train: 96.1%, val: 10.34%\n",
      "EPOCH: 516\n",
      "[LOSS] train: 9.058, val: 4.693\n",
      "[ACC%] train: 95.09%, val: 10.92%\n",
      "EPOCH: 517\n",
      "[LOSS] train: 8.98, val: 4.63\n",
      "[ACC%] train: 96.39%, val: 12.07%\n",
      "EPOCH: 518\n",
      "[LOSS] train: 9.084, val: 4.72\n",
      "[ACC%] train: 94.66%, val: 9.77%\n",
      "EPOCH: 519\n",
      "[LOSS] train: 9.034, val: 4.718\n",
      "[ACC%] train: 95.38%, val: 10.92%\n",
      "EPOCH: 520\n",
      "[LOSS] train: 9.055, val: 4.662\n",
      "[ACC%] train: 95.09%, val: 11.49%\n",
      "EPOCH: 521\n",
      "[LOSS] train: 9.061, val: 4.647\n",
      "[ACC%] train: 94.66%, val: 12.64%\n",
      "EPOCH: 522\n",
      "[LOSS] train: 8.956, val: 4.666\n",
      "[ACC%] train: 96.68%, val: 12.07%\n",
      "EPOCH: 523\n",
      "[LOSS] train: 8.978, val: 4.735\n",
      "[ACC%] train: 96.1%, val: 8.05%\n",
      "EPOCH: 524\n",
      "[LOSS] train: 8.987, val: 4.686\n",
      "[ACC%] train: 96.1%, val: 9.2%\n",
      "EPOCH: 525\n",
      "[LOSS] train: 9.005, val: 4.73\n",
      "[ACC%] train: 96.25%, val: 9.2%\n",
      "EPOCH: 526\n",
      "[LOSS] train: 8.956, val: 4.68\n",
      "[ACC%] train: 96.68%, val: 12.07%\n",
      "EPOCH: 527\n",
      "[LOSS] train: 8.989, val: 4.674\n",
      "[ACC%] train: 96.25%, val: 13.22%\n",
      "EPOCH: 528\n",
      "[LOSS] train: 9.046, val: 4.676\n",
      "[ACC%] train: 95.82%, val: 12.07%\n",
      "EPOCH: 529\n",
      "[LOSS] train: 8.935, val: 4.716\n",
      "[ACC%] train: 97.4%, val: 9.77%\n",
      "EPOCH: 530\n",
      "[LOSS] train: 8.968, val: 4.686\n",
      "[ACC%] train: 96.83%, val: 13.22%\n",
      "EPOCH: 531\n",
      "[LOSS] train: 9.038, val: 4.633\n",
      "[ACC%] train: 95.53%, val: 12.07%\n",
      "EPOCH: 532\n",
      "[LOSS] train: 9.02, val: 4.677\n",
      "[ACC%] train: 96.1%, val: 12.64%\n",
      "EPOCH: 533\n",
      "[LOSS] train: 8.946, val: 4.708\n",
      "[ACC%] train: 96.83%, val: 8.62%\n",
      "EPOCH: 534\n",
      "[LOSS] train: 9.039, val: 4.648\n",
      "[ACC%] train: 95.82%, val: 11.49%\n",
      "EPOCH: 535\n",
      "[LOSS] train: 9.049, val: 4.67\n",
      "[ACC%] train: 95.38%, val: 10.92%\n",
      "EPOCH: 536\n",
      "[LOSS] train: 9.075, val: 4.631\n",
      "[ACC%] train: 94.81%, val: 14.37%\n",
      "EPOCH: 537\n",
      "[LOSS] train: 8.977, val: 4.694\n",
      "[ACC%] train: 96.25%, val: 13.22%\n",
      "EPOCH: 538\n",
      "[LOSS] train: 8.959, val: 4.652\n",
      "[ACC%] train: 96.97%, val: 11.49%\n",
      "EPOCH: 539\n",
      "[LOSS] train: 8.992, val: 4.659\n",
      "[ACC%] train: 96.25%, val: 12.64%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 540\n",
      "[LOSS] train: 8.89, val: 4.658\n",
      "[ACC%] train: 97.84%, val: 11.49%\n",
      "EPOCH: 541\n",
      "[LOSS] train: 8.97, val: 4.699\n",
      "[ACC%] train: 96.39%, val: 10.92%\n",
      "EPOCH: 542\n",
      "[LOSS] train: 8.951, val: 4.633\n",
      "[ACC%] train: 96.54%, val: 10.92%\n",
      "EPOCH: 543\n",
      "[LOSS] train: 8.964, val: 4.668\n",
      "[ACC%] train: 97.11%, val: 9.77%\n",
      "EPOCH: 544\n",
      "[LOSS] train: 9.01, val: 4.618\n",
      "[ACC%] train: 95.96%, val: 13.22%\n",
      "EPOCH: 545\n",
      "[LOSS] train: 8.985, val: 4.654\n",
      "[ACC%] train: 97.11%, val: 13.22%\n",
      "EPOCH: 546\n",
      "[LOSS] train: 8.914, val: 4.723\n",
      "[ACC%] train: 97.84%, val: 10.92%\n",
      "EPOCH: 547\n",
      "[LOSS] train: 9.017, val: 4.705\n",
      "[ACC%] train: 96.1%, val: 9.77%\n",
      "EPOCH: 548\n",
      "[LOSS] train: 8.936, val: 4.659\n",
      "[ACC%] train: 96.97%, val: 13.22%\n",
      "EPOCH: 549\n",
      "[LOSS] train: 8.933, val: 4.684\n",
      "[ACC%] train: 96.97%, val: 10.92%\n",
      "EPOCH: 550\n",
      "[LOSS] train: 8.911, val: 4.697\n",
      "[ACC%] train: 97.98%, val: 11.49%\n",
      "EPOCH: 551\n",
      "[LOSS] train: 8.958, val: 4.671\n",
      "[ACC%] train: 97.11%, val: 11.49%\n",
      "EPOCH: 552\n",
      "[LOSS] train: 8.946, val: 4.675\n",
      "[ACC%] train: 97.26%, val: 10.92%\n",
      "EPOCH: 553\n",
      "[LOSS] train: 9.001, val: 4.664\n",
      "[ACC%] train: 96.1%, val: 12.07%\n",
      "EPOCH: 554\n",
      "[LOSS] train: 8.986, val: 4.643\n",
      "[ACC%] train: 96.25%, val: 12.07%\n",
      "EPOCH: 555\n",
      "[LOSS] train: 8.933, val: 4.662\n",
      "[ACC%] train: 97.55%, val: 10.34%\n",
      "EPOCH: 556\n",
      "[LOSS] train: 9.042, val: 4.72\n",
      "[ACC%] train: 95.82%, val: 9.77%\n",
      "EPOCH: 557\n",
      "[LOSS] train: 8.956, val: 4.734\n",
      "[ACC%] train: 96.68%, val: 8.05%\n",
      "EPOCH: 558\n",
      "[LOSS] train: 8.996, val: 4.674\n",
      "[ACC%] train: 96.39%, val: 9.2%\n",
      "EPOCH: 559\n",
      "[LOSS] train: 8.962, val: 4.661\n",
      "[ACC%] train: 96.54%, val: 10.92%\n",
      "EPOCH: 560\n",
      "[LOSS] train: 9.101, val: 4.636\n",
      "[ACC%] train: 94.66%, val: 12.64%\n",
      "EPOCH: 561\n",
      "[LOSS] train: 9.023, val: 4.751\n",
      "[ACC%] train: 95.96%, val: 9.2%\n",
      "EPOCH: 562\n",
      "[LOSS] train: 9.042, val: 4.766\n",
      "[ACC%] train: 95.67%, val: 9.2%\n",
      "EPOCH: 563\n",
      "[LOSS] train: 9.018, val: 4.797\n",
      "[ACC%] train: 95.82%, val: 6.9%\n",
      "EPOCH: 564\n",
      "[LOSS] train: 9.038, val: 4.676\n",
      "[ACC%] train: 95.96%, val: 8.62%\n",
      "EPOCH: 565\n",
      "[LOSS] train: 8.965, val: 4.695\n",
      "[ACC%] train: 96.68%, val: 9.2%\n",
      "EPOCH: 566\n",
      "[LOSS] train: 8.959, val: 4.691\n",
      "[ACC%] train: 96.68%, val: 12.07%\n",
      "EPOCH: 567\n",
      "[LOSS] train: 8.953, val: 4.687\n",
      "[ACC%] train: 96.97%, val: 12.07%\n",
      "EPOCH: 568\n",
      "[LOSS] train: 8.987, val: 4.72\n",
      "[ACC%] train: 96.39%, val: 9.77%\n",
      "EPOCH: 569\n",
      "[LOSS] train: 8.95, val: 4.68\n",
      "[ACC%] train: 97.11%, val: 9.2%\n",
      "EPOCH: 570\n",
      "[LOSS] train: 9.011, val: 4.72\n",
      "[ACC%] train: 95.96%, val: 9.2%\n",
      "EPOCH: 571\n",
      "[LOSS] train: 8.954, val: 4.739\n",
      "[ACC%] train: 97.26%, val: 9.77%\n",
      "EPOCH: 572\n",
      "[LOSS] train: 8.997, val: 4.7\n",
      "[ACC%] train: 95.96%, val: 9.2%\n",
      "EPOCH: 573\n",
      "[LOSS] train: 9.0, val: 4.703\n",
      "[ACC%] train: 96.54%, val: 10.34%\n",
      "EPOCH: 574\n",
      "[LOSS] train: 9.051, val: 4.736\n",
      "[ACC%] train: 95.53%, val: 8.62%\n",
      "EPOCH: 575\n",
      "[LOSS] train: 9.077, val: 4.68\n",
      "[ACC%] train: 95.38%, val: 10.92%\n",
      "EPOCH: 576\n",
      "[LOSS] train: 9.041, val: 4.699\n",
      "[ACC%] train: 95.24%, val: 9.77%\n",
      "EPOCH: 577\n",
      "[LOSS] train: 9.058, val: 4.626\n",
      "[ACC%] train: 95.09%, val: 12.07%\n",
      "EPOCH: 578\n",
      "[LOSS] train: 9.072, val: 4.64\n",
      "[ACC%] train: 94.52%, val: 9.2%\n",
      "EPOCH: 579\n",
      "[LOSS] train: 9.007, val: 4.623\n",
      "[ACC%] train: 95.96%, val: 13.22%\n",
      "EPOCH: 580\n",
      "[LOSS] train: 8.977, val: 4.623\n",
      "[ACC%] train: 97.26%, val: 14.94%\n",
      "EPOCH: 581\n",
      "[LOSS] train: 9.0, val: 4.746\n",
      "[ACC%] train: 95.67%, val: 9.77%\n",
      "EPOCH: 582\n",
      "[LOSS] train: 9.039, val: 4.654\n",
      "[ACC%] train: 95.53%, val: 10.92%\n",
      "EPOCH: 583\n",
      "[LOSS] train: 8.989, val: 4.628\n",
      "[ACC%] train: 96.25%, val: 10.92%\n",
      "EPOCH: 584\n",
      "[LOSS] train: 8.924, val: 4.67\n",
      "[ACC%] train: 97.26%, val: 12.07%\n",
      "EPOCH: 585\n",
      "[LOSS] train: 8.972, val: 4.713\n",
      "[ACC%] train: 96.1%, val: 9.2%\n",
      "EPOCH: 586\n",
      "[LOSS] train: 8.933, val: 4.758\n",
      "[ACC%] train: 97.26%, val: 8.05%\n",
      "EPOCH: 587\n",
      "[LOSS] train: 8.945, val: 4.715\n",
      "[ACC%] train: 96.54%, val: 9.77%\n",
      "EPOCH: 588\n",
      "[LOSS] train: 8.9, val: 4.72\n",
      "[ACC%] train: 97.69%, val: 9.2%\n",
      "EPOCH: 589\n",
      "[LOSS] train: 8.94, val: 4.629\n",
      "[ACC%] train: 97.11%, val: 13.79%\n",
      "EPOCH: 590\n",
      "[LOSS] train: 9.01, val: 4.645\n",
      "[ACC%] train: 95.67%, val: 11.49%\n",
      "EPOCH: 591\n",
      "[LOSS] train: 9.045, val: 4.694\n",
      "[ACC%] train: 95.38%, val: 9.77%\n",
      "EPOCH: 592\n",
      "[LOSS] train: 9.11, val: 4.729\n",
      "[ACC%] train: 94.23%, val: 9.77%\n",
      "EPOCH: 593\n",
      "[LOSS] train: 9.044, val: 4.594\n",
      "[ACC%] train: 95.09%, val: 12.64%\n",
      "EPOCH: 594\n",
      "[LOSS] train: 9.029, val: 4.68\n",
      "[ACC%] train: 95.53%, val: 13.79%\n",
      "EPOCH: 595\n",
      "[LOSS] train: 9.108, val: 4.695\n",
      "[ACC%] train: 95.09%, val: 9.77%\n",
      "EPOCH: 596\n",
      "[LOSS] train: 9.041, val: 4.707\n",
      "[ACC%] train: 95.53%, val: 10.34%\n",
      "EPOCH: 597\n",
      "[LOSS] train: 9.116, val: 4.73\n",
      "[ACC%] train: 94.81%, val: 7.47%\n",
      "EPOCH: 598\n",
      "[LOSS] train: 8.995, val: 4.706\n",
      "[ACC%] train: 96.39%, val: 8.62%\n",
      "EPOCH: 599\n",
      "[LOSS] train: 9.019, val: 4.687\n",
      "[ACC%] train: 96.39%, val: 9.77%\n",
      "EPOCH: 600\n",
      "[LOSS] train: 9.041, val: 4.725\n",
      "[ACC%] train: 95.24%, val: 8.62%\n",
      "EPOCH: 601\n",
      "[LOSS] train: 8.98, val: 4.714\n",
      "[ACC%] train: 96.54%, val: 10.34%\n",
      "EPOCH: 602\n",
      "[LOSS] train: 8.984, val: 4.675\n",
      "[ACC%] train: 96.39%, val: 9.77%\n",
      "EPOCH: 603\n",
      "[LOSS] train: 8.977, val: 4.627\n",
      "[ACC%] train: 96.39%, val: 12.64%\n",
      "EPOCH: 604\n",
      "[LOSS] train: 9.023, val: 4.665\n",
      "[ACC%] train: 95.38%, val: 11.49%\n",
      "EPOCH: 605\n",
      "[LOSS] train: 8.968, val: 4.67\n",
      "[ACC%] train: 96.68%, val: 10.92%\n",
      "EPOCH: 606\n",
      "[LOSS] train: 8.934, val: 4.719\n",
      "[ACC%] train: 97.4%, val: 9.77%\n",
      "EPOCH: 607\n",
      "[LOSS] train: 8.998, val: 4.679\n",
      "[ACC%] train: 96.54%, val: 12.07%\n",
      "EPOCH: 608\n",
      "[LOSS] train: 8.956, val: 4.724\n",
      "[ACC%] train: 96.83%, val: 10.92%\n",
      "EPOCH: 609\n",
      "[LOSS] train: 8.98, val: 4.723\n",
      "[ACC%] train: 96.83%, val: 8.62%\n",
      "EPOCH: 610\n",
      "[LOSS] train: 8.984, val: 4.709\n",
      "[ACC%] train: 96.54%, val: 10.34%\n",
      "EPOCH: 611\n",
      "[LOSS] train: 8.956, val: 4.702\n",
      "[ACC%] train: 96.97%, val: 9.2%\n",
      "EPOCH: 612\n",
      "[LOSS] train: 8.966, val: 4.728\n",
      "[ACC%] train: 96.39%, val: 8.05%\n",
      "EPOCH: 613\n",
      "[LOSS] train: 9.065, val: 4.677\n",
      "[ACC%] train: 94.81%, val: 10.34%\n",
      "EPOCH: 614\n",
      "[LOSS] train: 8.963, val: 4.631\n",
      "[ACC%] train: 96.25%, val: 12.07%\n",
      "EPOCH: 615\n",
      "[LOSS] train: 9.022, val: 4.697\n",
      "[ACC%] train: 95.96%, val: 11.49%\n",
      "EPOCH: 616\n",
      "[LOSS] train: 9.016, val: 4.684\n",
      "[ACC%] train: 95.96%, val: 11.49%\n",
      "EPOCH: 617\n",
      "[LOSS] train: 9.071, val: 4.659\n",
      "[ACC%] train: 94.37%, val: 11.49%\n",
      "EPOCH: 618\n",
      "[LOSS] train: 8.947, val: 4.678\n",
      "[ACC%] train: 96.83%, val: 9.77%\n",
      "EPOCH: 619\n",
      "[LOSS] train: 9.009, val: 4.653\n",
      "[ACC%] train: 96.1%, val: 12.07%\n",
      "EPOCH: 620\n",
      "[LOSS] train: 9.035, val: 4.626\n",
      "[ACC%] train: 95.67%, val: 11.49%\n",
      "EPOCH: 621\n",
      "[LOSS] train: 8.949, val: 4.625\n",
      "[ACC%] train: 97.26%, val: 11.49%\n",
      "EPOCH: 622\n",
      "[LOSS] train: 8.993, val: 4.636\n",
      "[ACC%] train: 96.39%, val: 12.07%\n",
      "EPOCH: 623\n",
      "[LOSS] train: 8.977, val: 4.634\n",
      "[ACC%] train: 96.39%, val: 11.49%\n",
      "EPOCH: 624\n",
      "[LOSS] train: 8.985, val: 4.686\n",
      "[ACC%] train: 96.54%, val: 12.07%\n",
      "EPOCH: 625\n",
      "[LOSS] train: 8.971, val: 4.683\n",
      "[ACC%] train: 96.25%, val: 9.77%\n",
      "EPOCH: 626\n",
      "[LOSS] train: 8.909, val: 4.706\n",
      "[ACC%] train: 97.69%, val: 10.34%\n",
      "EPOCH: 627\n",
      "[LOSS] train: 8.956, val: 4.712\n",
      "[ACC%] train: 96.97%, val: 10.34%\n",
      "EPOCH: 628\n",
      "[LOSS] train: 8.915, val: 4.741\n",
      "[ACC%] train: 97.4%, val: 9.2%\n",
      "EPOCH: 629\n",
      "[LOSS] train: 8.92, val: 4.708\n",
      "[ACC%] train: 97.69%, val: 9.77%\n",
      "EPOCH: 630\n",
      "[LOSS] train: 9.022, val: 4.712\n",
      "[ACC%] train: 95.53%, val: 9.77%\n",
      "EPOCH: 631\n",
      "[LOSS] train: 8.993, val: 4.728\n",
      "[ACC%] train: 96.39%, val: 8.05%\n",
      "EPOCH: 632\n",
      "[LOSS] train: 9.091, val: 4.698\n",
      "[ACC%] train: 94.37%, val: 9.2%\n",
      "EPOCH: 633\n",
      "[LOSS] train: 9.06, val: 4.67\n",
      "[ACC%] train: 94.66%, val: 10.92%\n",
      "EPOCH: 634\n",
      "[LOSS] train: 8.979, val: 4.681\n",
      "[ACC%] train: 96.25%, val: 10.34%\n",
      "EPOCH: 635\n",
      "[LOSS] train: 8.93, val: 4.713\n",
      "[ACC%] train: 97.55%, val: 11.49%\n",
      "EPOCH: 636\n",
      "[LOSS] train: 9.014, val: 4.727\n",
      "[ACC%] train: 96.25%, val: 9.77%\n",
      "EPOCH: 637\n",
      "[LOSS] train: 9.002, val: 4.693\n",
      "[ACC%] train: 95.67%, val: 10.92%\n",
      "EPOCH: 638\n",
      "[LOSS] train: 9.01, val: 4.636\n",
      "[ACC%] train: 95.67%, val: 11.49%\n",
      "EPOCH: 639\n",
      "[LOSS] train: 9.025, val: 4.679\n",
      "[ACC%] train: 95.53%, val: 10.92%\n",
      "EPOCH: 640\n",
      "[LOSS] train: 8.986, val: 4.769\n",
      "[ACC%] train: 96.68%, val: 8.05%\n",
      "EPOCH: 641\n",
      "[LOSS] train: 9.208, val: 4.744\n",
      "[ACC%] train: 93.22%, val: 9.2%\n",
      "EPOCH: 642\n",
      "[LOSS] train: 9.112, val: 4.682\n",
      "[ACC%] train: 94.37%, val: 10.34%\n",
      "EPOCH: 643\n",
      "[LOSS] train: 9.028, val: 4.695\n",
      "[ACC%] train: 96.1%, val: 10.34%\n",
      "EPOCH: 644\n",
      "[LOSS] train: 9.162, val: 4.703\n",
      "[ACC%] train: 92.35%, val: 8.62%\n",
      "EPOCH: 645\n",
      "[LOSS] train: 9.085, val: 4.714\n",
      "[ACC%] train: 95.24%, val: 9.2%\n",
      "EPOCH: 646\n",
      "[LOSS] train: 8.991, val: 4.673\n",
      "[ACC%] train: 96.1%, val: 10.34%\n",
      "EPOCH: 647\n",
      "[LOSS] train: 9.075, val: 4.728\n",
      "[ACC%] train: 94.52%, val: 8.05%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 648\n",
      "[LOSS] train: 9.027, val: 4.74\n",
      "[ACC%] train: 96.1%, val: 7.47%\n",
      "EPOCH: 649\n",
      "[LOSS] train: 9.025, val: 4.696\n",
      "[ACC%] train: 95.82%, val: 9.77%\n",
      "EPOCH: 650\n",
      "[LOSS] train: 9.151, val: 4.773\n",
      "[ACC%] train: 94.08%, val: 8.62%\n",
      "EPOCH: 651\n",
      "[LOSS] train: 9.103, val: 4.763\n",
      "[ACC%] train: 94.81%, val: 6.9%\n",
      "EPOCH: 652\n",
      "[LOSS] train: 9.044, val: 4.78\n",
      "[ACC%] train: 95.67%, val: 5.17%\n",
      "EPOCH: 653\n",
      "[LOSS] train: 9.1, val: 4.704\n",
      "[ACC%] train: 94.37%, val: 9.77%\n",
      "EPOCH: 654\n",
      "[LOSS] train: 8.97, val: 4.691\n",
      "[ACC%] train: 96.39%, val: 12.07%\n",
      "EPOCH: 655\n",
      "[LOSS] train: 9.015, val: 4.73\n",
      "[ACC%] train: 96.25%, val: 10.92%\n",
      "EPOCH: 656\n",
      "[LOSS] train: 9.072, val: 4.748\n",
      "[ACC%] train: 94.81%, val: 6.32%\n",
      "EPOCH: 657\n",
      "[LOSS] train: 9.102, val: 4.712\n",
      "[ACC%] train: 94.37%, val: 8.05%\n",
      "EPOCH: 658\n",
      "[LOSS] train: 9.015, val: 4.703\n",
      "[ACC%] train: 95.53%, val: 8.62%\n",
      "EPOCH: 659\n",
      "[LOSS] train: 9.047, val: 4.715\n",
      "[ACC%] train: 95.09%, val: 10.34%\n",
      "EPOCH: 660\n",
      "[LOSS] train: 9.063, val: 4.713\n",
      "[ACC%] train: 95.38%, val: 7.47%\n",
      "EPOCH: 661\n",
      "[LOSS] train: 9.01, val: 4.739\n",
      "[ACC%] train: 95.82%, val: 7.47%\n",
      "EPOCH: 662\n",
      "[LOSS] train: 9.016, val: 4.752\n",
      "[ACC%] train: 95.38%, val: 8.62%\n",
      "EPOCH: 663\n",
      "[LOSS] train: 9.11, val: 4.7\n",
      "[ACC%] train: 94.23%, val: 8.05%\n",
      "EPOCH: 664\n",
      "[LOSS] train: 9.084, val: 4.712\n",
      "[ACC%] train: 94.66%, val: 8.05%\n",
      "EPOCH: 665\n",
      "[LOSS] train: 9.033, val: 4.628\n",
      "[ACC%] train: 95.53%, val: 11.49%\n",
      "EPOCH: 666\n",
      "[LOSS] train: 8.99, val: 4.721\n",
      "[ACC%] train: 96.83%, val: 9.77%\n",
      "EPOCH: 667\n",
      "[LOSS] train: 9.089, val: 4.761\n",
      "[ACC%] train: 94.81%, val: 8.62%\n",
      "EPOCH: 668\n",
      "[LOSS] train: 8.931, val: 4.736\n",
      "[ACC%] train: 97.11%, val: 8.05%\n",
      "EPOCH: 669\n",
      "[LOSS] train: 9.026, val: 4.711\n",
      "[ACC%] train: 96.1%, val: 10.34%\n",
      "EPOCH: 670\n",
      "[LOSS] train: 9.061, val: 4.661\n",
      "[ACC%] train: 95.53%, val: 10.34%\n",
      "EPOCH: 671\n",
      "[LOSS] train: 9.024, val: 4.759\n",
      "[ACC%] train: 96.25%, val: 8.05%\n",
      "EPOCH: 672\n",
      "[LOSS] train: 8.986, val: 4.735\n",
      "[ACC%] train: 96.83%, val: 6.9%\n",
      "EPOCH: 673\n",
      "[LOSS] train: 8.893, val: 4.708\n",
      "[ACC%] train: 97.69%, val: 9.77%\n",
      "EPOCH: 674\n",
      "[LOSS] train: 8.997, val: 4.727\n",
      "[ACC%] train: 96.1%, val: 10.92%\n",
      "EPOCH: 675\n",
      "[LOSS] train: 9.004, val: 4.685\n",
      "[ACC%] train: 95.82%, val: 8.62%\n",
      "EPOCH: 676\n",
      "[LOSS] train: 9.043, val: 4.728\n",
      "[ACC%] train: 95.53%, val: 8.05%\n",
      "EPOCH: 677\n",
      "[LOSS] train: 8.929, val: 4.733\n",
      "[ACC%] train: 97.26%, val: 8.05%\n",
      "EPOCH: 678\n",
      "[LOSS] train: 9.078, val: 4.73\n",
      "[ACC%] train: 94.81%, val: 6.9%\n",
      "EPOCH: 679\n",
      "[LOSS] train: 8.99, val: 4.721\n",
      "[ACC%] train: 96.25%, val: 9.2%\n",
      "EPOCH: 680\n",
      "[LOSS] train: 8.969, val: 4.717\n",
      "[ACC%] train: 96.25%, val: 11.49%\n",
      "EPOCH: 681\n",
      "[LOSS] train: 8.966, val: 4.672\n",
      "[ACC%] train: 96.39%, val: 10.34%\n",
      "EPOCH: 682\n",
      "[LOSS] train: 8.996, val: 4.745\n",
      "[ACC%] train: 96.68%, val: 8.62%\n",
      "EPOCH: 683\n",
      "[LOSS] train: 8.925, val: 4.718\n",
      "[ACC%] train: 97.55%, val: 9.77%\n",
      "EPOCH: 684\n",
      "[LOSS] train: 8.964, val: 4.63\n",
      "[ACC%] train: 96.83%, val: 13.22%\n",
      "EPOCH: 685\n",
      "[LOSS] train: 8.986, val: 4.593\n",
      "[ACC%] train: 96.25%, val: 12.64%\n",
      "EPOCH: 686\n",
      "[LOSS] train: 8.964, val: 4.649\n",
      "[ACC%] train: 96.39%, val: 12.07%\n",
      "EPOCH: 687\n",
      "[LOSS] train: 8.979, val: 4.695\n",
      "[ACC%] train: 96.83%, val: 10.92%\n",
      "EPOCH: 688\n",
      "[LOSS] train: 8.953, val: 4.727\n",
      "[ACC%] train: 97.4%, val: 10.34%\n",
      "EPOCH: 689\n",
      "[LOSS] train: 8.938, val: 4.644\n",
      "[ACC%] train: 96.83%, val: 9.2%\n",
      "EPOCH: 690\n",
      "[LOSS] train: 8.963, val: 4.65\n",
      "[ACC%] train: 96.68%, val: 13.22%\n",
      "EPOCH: 691\n",
      "[LOSS] train: 8.949, val: 4.69\n",
      "[ACC%] train: 96.68%, val: 11.49%\n",
      "EPOCH: 692\n",
      "[LOSS] train: 8.997, val: 4.703\n",
      "[ACC%] train: 96.25%, val: 11.49%\n",
      "EPOCH: 693\n",
      "[LOSS] train: 8.922, val: 4.696\n",
      "[ACC%] train: 97.4%, val: 10.92%\n",
      "EPOCH: 694\n",
      "[LOSS] train: 8.946, val: 4.668\n",
      "[ACC%] train: 97.11%, val: 12.64%\n",
      "EPOCH: 695\n",
      "[LOSS] train: 8.969, val: 4.708\n",
      "[ACC%] train: 96.83%, val: 9.77%\n",
      "EPOCH: 696\n",
      "[LOSS] train: 8.962, val: 4.696\n",
      "[ACC%] train: 96.83%, val: 9.2%\n",
      "EPOCH: 697\n",
      "[LOSS] train: 8.958, val: 4.698\n",
      "[ACC%] train: 96.39%, val: 9.77%\n",
      "EPOCH: 698\n",
      "[LOSS] train: 8.952, val: 4.701\n",
      "[ACC%] train: 96.39%, val: 9.2%\n",
      "EPOCH: 699\n",
      "[LOSS] train: 8.923, val: 4.756\n",
      "[ACC%] train: 97.4%, val: 9.77%\n",
      "EPOCH: 700\n",
      "[LOSS] train: 9.006, val: 4.656\n",
      "[ACC%] train: 96.39%, val: 12.64%\n",
      "EPOCH: 701\n",
      "[LOSS] train: 8.979, val: 4.673\n",
      "[ACC%] train: 96.54%, val: 9.77%\n",
      "EPOCH: 702\n",
      "[LOSS] train: 8.945, val: 4.691\n",
      "[ACC%] train: 97.26%, val: 12.07%\n",
      "EPOCH: 703\n",
      "[LOSS] train: 8.964, val: 4.667\n",
      "[ACC%] train: 96.54%, val: 10.92%\n",
      "EPOCH: 704\n",
      "[LOSS] train: 8.993, val: 4.616\n",
      "[ACC%] train: 95.82%, val: 12.07%\n",
      "EPOCH: 705\n",
      "[LOSS] train: 8.981, val: 4.677\n",
      "[ACC%] train: 96.68%, val: 11.49%\n",
      "EPOCH: 706\n",
      "[LOSS] train: 9.008, val: 4.666\n",
      "[ACC%] train: 95.67%, val: 13.79%\n",
      "EPOCH: 707\n",
      "[LOSS] train: 9.007, val: 4.663\n",
      "[ACC%] train: 95.67%, val: 12.07%\n",
      "EPOCH: 708\n",
      "[LOSS] train: 8.956, val: 4.657\n",
      "[ACC%] train: 97.11%, val: 12.64%\n",
      "EPOCH: 709\n",
      "[LOSS] train: 8.952, val: 4.667\n",
      "[ACC%] train: 96.68%, val: 12.64%\n",
      "EPOCH: 710\n",
      "[LOSS] train: 8.978, val: 4.692\n",
      "[ACC%] train: 96.25%, val: 9.2%\n",
      "EPOCH: 711\n",
      "[LOSS] train: 9.03, val: 4.737\n",
      "[ACC%] train: 95.96%, val: 8.05%\n",
      "EPOCH: 712\n",
      "[LOSS] train: 8.96, val: 4.731\n",
      "[ACC%] train: 96.83%, val: 8.05%\n",
      "EPOCH: 713\n",
      "[LOSS] train: 8.926, val: 4.718\n",
      "[ACC%] train: 97.11%, val: 9.77%\n",
      "EPOCH: 714\n",
      "[LOSS] train: 8.936, val: 4.731\n",
      "[ACC%] train: 96.83%, val: 11.49%\n",
      "EPOCH: 715\n",
      "[LOSS] train: 9.016, val: 4.746\n",
      "[ACC%] train: 95.67%, val: 9.77%\n",
      "EPOCH: 716\n",
      "[LOSS] train: 9.068, val: 4.729\n",
      "[ACC%] train: 94.66%, val: 9.77%\n",
      "EPOCH: 717\n",
      "[LOSS] train: 9.081, val: 4.735\n",
      "[ACC%] train: 94.95%, val: 9.77%\n",
      "EPOCH: 718\n",
      "[LOSS] train: 9.0, val: 4.674\n",
      "[ACC%] train: 96.39%, val: 10.34%\n",
      "EPOCH: 719\n",
      "[LOSS] train: 8.931, val: 4.666\n",
      "[ACC%] train: 96.97%, val: 12.07%\n",
      "EPOCH: 720\n",
      "[LOSS] train: 9.004, val: 4.632\n",
      "[ACC%] train: 95.67%, val: 12.07%\n",
      "EPOCH: 721\n",
      "[LOSS] train: 9.099, val: 4.681\n",
      "[ACC%] train: 94.23%, val: 9.2%\n",
      "EPOCH: 722\n",
      "[LOSS] train: 9.012, val: 4.732\n",
      "[ACC%] train: 96.54%, val: 10.34%\n",
      "EPOCH: 723\n",
      "[LOSS] train: 9.01, val: 4.761\n",
      "[ACC%] train: 96.1%, val: 7.47%\n",
      "EPOCH: 724\n",
      "[LOSS] train: 8.941, val: 4.7\n",
      "[ACC%] train: 97.69%, val: 10.34%\n",
      "EPOCH: 725\n",
      "[LOSS] train: 8.984, val: 4.703\n",
      "[ACC%] train: 96.39%, val: 10.34%\n",
      "EPOCH: 726\n",
      "[LOSS] train: 8.933, val: 4.709\n",
      "[ACC%] train: 97.84%, val: 10.92%\n",
      "EPOCH: 727\n",
      "[LOSS] train: 9.067, val: 4.73\n",
      "[ACC%] train: 95.67%, val: 10.34%\n",
      "EPOCH: 728\n",
      "[LOSS] train: 9.114, val: 4.737\n",
      "[ACC%] train: 93.65%, val: 8.05%\n",
      "EPOCH: 729\n",
      "[LOSS] train: 9.076, val: 4.663\n",
      "[ACC%] train: 95.53%, val: 9.2%\n",
      "EPOCH: 730\n",
      "[LOSS] train: 9.043, val: 4.686\n",
      "[ACC%] train: 95.24%, val: 10.92%\n",
      "EPOCH: 731\n",
      "[LOSS] train: 9.023, val: 4.701\n",
      "[ACC%] train: 95.53%, val: 9.77%\n",
      "EPOCH: 732\n",
      "[LOSS] train: 8.96, val: 4.687\n",
      "[ACC%] train: 96.68%, val: 9.77%\n",
      "EPOCH: 733\n",
      "[LOSS] train: 8.977, val: 4.657\n",
      "[ACC%] train: 95.96%, val: 10.92%\n",
      "EPOCH: 734\n",
      "[LOSS] train: 8.959, val: 4.682\n",
      "[ACC%] train: 96.97%, val: 11.49%\n",
      "EPOCH: 735\n",
      "[LOSS] train: 9.05, val: 4.66\n",
      "[ACC%] train: 95.67%, val: 13.22%\n",
      "EPOCH: 736\n",
      "[LOSS] train: 9.013, val: 4.664\n",
      "[ACC%] train: 96.1%, val: 10.92%\n",
      "EPOCH: 737\n",
      "[LOSS] train: 8.99, val: 4.613\n",
      "[ACC%] train: 96.25%, val: 12.07%\n",
      "EPOCH: 738\n",
      "[LOSS] train: 8.957, val: 4.681\n",
      "[ACC%] train: 97.26%, val: 12.64%\n",
      "EPOCH: 739\n",
      "[LOSS] train: 9.008, val: 4.723\n",
      "[ACC%] train: 96.25%, val: 8.62%\n",
      "EPOCH: 740\n",
      "[LOSS] train: 9.044, val: 4.716\n",
      "[ACC%] train: 95.53%, val: 10.92%\n",
      "EPOCH: 741\n",
      "[LOSS] train: 9.079, val: 4.645\n",
      "[ACC%] train: 94.95%, val: 12.07%\n",
      "EPOCH: 742\n",
      "[LOSS] train: 9.105, val: 4.67\n",
      "[ACC%] train: 94.52%, val: 10.92%\n",
      "EPOCH: 743\n",
      "[LOSS] train: 9.021, val: 4.698\n",
      "[ACC%] train: 95.53%, val: 10.34%\n",
      "EPOCH: 744\n",
      "[LOSS] train: 9.077, val: 4.595\n",
      "[ACC%] train: 94.66%, val: 14.37%\n",
      "EPOCH: 745\n",
      "[LOSS] train: 9.041, val: 4.721\n",
      "[ACC%] train: 95.24%, val: 10.92%\n",
      "EPOCH: 746\n",
      "[LOSS] train: 8.969, val: 4.747\n",
      "[ACC%] train: 96.83%, val: 9.77%\n",
      "EPOCH: 747\n",
      "[LOSS] train: 9.039, val: 4.67\n",
      "[ACC%] train: 95.67%, val: 11.49%\n",
      "EPOCH: 748\n",
      "[LOSS] train: 8.982, val: 4.709\n",
      "[ACC%] train: 96.25%, val: 10.92%\n",
      "EPOCH: 749\n",
      "[LOSS] train: 8.98, val: 4.676\n",
      "[ACC%] train: 96.68%, val: 11.49%\n",
      "EPOCH: 750\n",
      "[LOSS] train: 9.101, val: 4.685\n",
      "[ACC%] train: 94.37%, val: 12.64%\n",
      "EPOCH: 751\n",
      "[LOSS] train: 8.972, val: 4.694\n",
      "[ACC%] train: 96.25%, val: 10.34%\n",
      "EPOCH: 752\n",
      "[LOSS] train: 9.029, val: 4.678\n",
      "[ACC%] train: 95.96%, val: 8.62%\n",
      "EPOCH: 753\n",
      "[LOSS] train: 9.029, val: 4.712\n",
      "[ACC%] train: 95.96%, val: 10.34%\n",
      "EPOCH: 754\n",
      "[LOSS] train: 8.969, val: 4.637\n",
      "[ACC%] train: 96.25%, val: 10.34%\n",
      "EPOCH: 755\n",
      "[LOSS] train: 8.941, val: 4.63\n",
      "[ACC%] train: 97.26%, val: 13.79%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 756\n",
      "[LOSS] train: 8.908, val: 4.655\n",
      "[ACC%] train: 97.4%, val: 12.07%\n",
      "EPOCH: 757\n",
      "[LOSS] train: 8.946, val: 4.619\n",
      "[ACC%] train: 97.11%, val: 13.79%\n",
      "EPOCH: 758\n",
      "[LOSS] train: 8.965, val: 4.657\n",
      "[ACC%] train: 96.97%, val: 12.07%\n",
      "EPOCH: 759\n",
      "[LOSS] train: 8.945, val: 4.659\n",
      "[ACC%] train: 96.83%, val: 11.49%\n",
      "EPOCH: 760\n",
      "[LOSS] train: 9.031, val: 4.659\n",
      "[ACC%] train: 95.53%, val: 11.49%\n",
      "EPOCH: 761\n",
      "[LOSS] train: 9.027, val: 4.677\n",
      "[ACC%] train: 95.24%, val: 12.07%\n",
      "EPOCH: 762\n",
      "[LOSS] train: 9.104, val: 4.695\n",
      "[ACC%] train: 94.08%, val: 9.77%\n",
      "EPOCH: 763\n",
      "[LOSS] train: 8.997, val: 4.761\n",
      "[ACC%] train: 95.53%, val: 8.62%\n",
      "EPOCH: 764\n",
      "[LOSS] train: 9.12, val: 4.702\n",
      "[ACC%] train: 94.23%, val: 8.62%\n",
      "EPOCH: 765\n",
      "[LOSS] train: 9.044, val: 4.592\n",
      "[ACC%] train: 94.95%, val: 13.79%\n",
      "EPOCH: 766\n",
      "[LOSS] train: 9.031, val: 4.705\n",
      "[ACC%] train: 96.1%, val: 12.07%\n",
      "EPOCH: 767\n",
      "[LOSS] train: 9.011, val: 4.607\n",
      "[ACC%] train: 96.25%, val: 13.79%\n",
      "EPOCH: 768\n",
      "[LOSS] train: 9.04, val: 4.702\n",
      "[ACC%] train: 95.53%, val: 9.2%\n",
      "EPOCH: 769\n",
      "[LOSS] train: 8.975, val: 4.693\n",
      "[ACC%] train: 96.54%, val: 11.49%\n",
      "EPOCH: 770\n",
      "[LOSS] train: 8.986, val: 4.677\n",
      "[ACC%] train: 96.39%, val: 10.92%\n",
      "EPOCH: 771\n",
      "[LOSS] train: 8.966, val: 4.685\n",
      "[ACC%] train: 96.97%, val: 11.49%\n",
      "EPOCH: 772\n",
      "[LOSS] train: 9.026, val: 4.72\n",
      "[ACC%] train: 95.82%, val: 10.34%\n",
      "EPOCH: 773\n",
      "[LOSS] train: 8.96, val: 4.713\n",
      "[ACC%] train: 96.68%, val: 11.49%\n",
      "EPOCH: 774\n",
      "[LOSS] train: 9.011, val: 4.667\n",
      "[ACC%] train: 95.53%, val: 10.34%\n",
      "EPOCH: 775\n",
      "[LOSS] train: 8.989, val: 4.703\n",
      "[ACC%] train: 96.39%, val: 9.2%\n",
      "EPOCH: 776\n",
      "[LOSS] train: 9.009, val: 4.718\n",
      "[ACC%] train: 95.96%, val: 11.49%\n",
      "EPOCH: 777\n",
      "[LOSS] train: 9.153, val: 4.68\n",
      "[ACC%] train: 93.51%, val: 10.92%\n",
      "EPOCH: 778\n",
      "[LOSS] train: 8.996, val: 4.578\n",
      "[ACC%] train: 96.54%, val: 14.37%\n",
      "EPOCH: 779\n",
      "[LOSS] train: 9.136, val: 4.676\n",
      "[ACC%] train: 93.94%, val: 12.64%\n",
      "EPOCH: 780\n",
      "[LOSS] train: 8.958, val: 4.704\n",
      "[ACC%] train: 96.97%, val: 8.62%\n",
      "EPOCH: 781\n",
      "[LOSS] train: 8.892, val: 4.684\n",
      "[ACC%] train: 97.69%, val: 12.07%\n",
      "EPOCH: 782\n",
      "[LOSS] train: 9.048, val: 4.66\n",
      "[ACC%] train: 95.09%, val: 13.22%\n",
      "EPOCH: 783\n",
      "[LOSS] train: 9.013, val: 4.613\n",
      "[ACC%] train: 96.1%, val: 15.52%\n",
      "EPOCH: 784\n",
      "[LOSS] train: 9.033, val: 4.675\n",
      "[ACC%] train: 95.82%, val: 12.07%\n",
      "EPOCH: 785\n",
      "[LOSS] train: 9.013, val: 4.62\n",
      "[ACC%] train: 95.96%, val: 14.94%\n",
      "EPOCH: 786\n",
      "[LOSS] train: 9.052, val: 4.685\n",
      "[ACC%] train: 95.53%, val: 12.07%\n",
      "EPOCH: 787\n",
      "[LOSS] train: 9.008, val: 4.691\n",
      "[ACC%] train: 95.96%, val: 10.34%\n",
      "EPOCH: 788\n",
      "[LOSS] train: 8.97, val: 4.696\n",
      "[ACC%] train: 96.39%, val: 12.07%\n",
      "EPOCH: 789\n",
      "[LOSS] train: 9.002, val: 4.649\n",
      "[ACC%] train: 95.82%, val: 13.79%\n",
      "EPOCH: 790\n",
      "[LOSS] train: 9.105, val: 4.722\n",
      "[ACC%] train: 94.37%, val: 10.92%\n",
      "EPOCH: 791\n",
      "[LOSS] train: 8.975, val: 4.727\n",
      "[ACC%] train: 96.68%, val: 8.05%\n",
      "EPOCH: 792\n",
      "[LOSS] train: 9.133, val: 4.721\n",
      "[ACC%] train: 94.52%, val: 10.34%\n",
      "EPOCH: 793\n",
      "[LOSS] train: 9.143, val: 4.665\n",
      "[ACC%] train: 93.07%, val: 11.49%\n",
      "EPOCH: 794\n",
      "[LOSS] train: 9.103, val: 4.748\n",
      "[ACC%] train: 94.81%, val: 9.77%\n",
      "EPOCH: 795\n",
      "[LOSS] train: 9.072, val: 4.689\n",
      "[ACC%] train: 95.09%, val: 11.49%\n",
      "EPOCH: 796\n",
      "[LOSS] train: 9.12, val: 4.646\n",
      "[ACC%] train: 94.52%, val: 10.92%\n",
      "EPOCH: 797\n",
      "[LOSS] train: 9.076, val: 4.637\n",
      "[ACC%] train: 95.09%, val: 12.07%\n",
      "EPOCH: 798\n",
      "[LOSS] train: 9.068, val: 4.767\n",
      "[ACC%] train: 95.38%, val: 10.34%\n",
      "EPOCH: 799\n",
      "[LOSS] train: 9.112, val: 4.686\n",
      "[ACC%] train: 94.23%, val: 12.07%\n",
      "EPOCH: 800\n",
      "[LOSS] train: 9.147, val: 4.66\n",
      "[ACC%] train: 93.36%, val: 11.49%\n",
      "EPOCH: 801\n",
      "[LOSS] train: 9.177, val: 4.683\n",
      "[ACC%] train: 92.93%, val: 10.92%\n",
      "EPOCH: 802\n",
      "[LOSS] train: 9.133, val: 4.721\n",
      "[ACC%] train: 93.8%, val: 10.34%\n",
      "EPOCH: 803\n",
      "[LOSS] train: 8.988, val: 4.662\n",
      "[ACC%] train: 96.1%, val: 9.77%\n",
      "EPOCH: 804\n",
      "[LOSS] train: 9.21, val: 4.652\n",
      "[ACC%] train: 91.92%, val: 13.79%\n",
      "EPOCH: 805\n",
      "[LOSS] train: 9.127, val: 4.62\n",
      "[ACC%] train: 93.65%, val: 13.79%\n",
      "EPOCH: 806\n",
      "[LOSS] train: 8.986, val: 4.693\n",
      "[ACC%] train: 96.39%, val: 11.49%\n",
      "EPOCH: 807\n",
      "[LOSS] train: 9.045, val: 4.684\n",
      "[ACC%] train: 95.09%, val: 9.77%\n",
      "EPOCH: 808\n",
      "[LOSS] train: 8.967, val: 4.702\n",
      "[ACC%] train: 96.97%, val: 9.2%\n",
      "EPOCH: 809\n",
      "[LOSS] train: 9.053, val: 4.692\n",
      "[ACC%] train: 95.24%, val: 10.34%\n",
      "EPOCH: 810\n",
      "[LOSS] train: 9.043, val: 4.723\n",
      "[ACC%] train: 95.24%, val: 10.34%\n",
      "EPOCH: 811\n",
      "[LOSS] train: 8.967, val: 4.675\n",
      "[ACC%] train: 96.54%, val: 12.07%\n",
      "EPOCH: 812\n",
      "[LOSS] train: 9.035, val: 4.76\n",
      "[ACC%] train: 95.67%, val: 8.62%\n",
      "EPOCH: 813\n",
      "[LOSS] train: 9.039, val: 4.695\n",
      "[ACC%] train: 95.24%, val: 10.34%\n",
      "EPOCH: 814\n",
      "[LOSS] train: 9.047, val: 4.641\n",
      "[ACC%] train: 95.38%, val: 10.92%\n",
      "EPOCH: 815\n",
      "[LOSS] train: 8.957, val: 4.641\n",
      "[ACC%] train: 97.4%, val: 12.07%\n",
      "EPOCH: 816\n",
      "[LOSS] train: 9.013, val: 4.71\n",
      "[ACC%] train: 95.96%, val: 12.07%\n",
      "EPOCH: 817\n",
      "[LOSS] train: 9.062, val: 4.685\n",
      "[ACC%] train: 94.37%, val: 12.64%\n",
      "EPOCH: 818\n",
      "[LOSS] train: 9.019, val: 4.684\n",
      "[ACC%] train: 95.53%, val: 11.49%\n",
      "EPOCH: 819\n",
      "[LOSS] train: 9.041, val: 4.764\n",
      "[ACC%] train: 95.67%, val: 9.2%\n",
      "EPOCH: 820\n",
      "[LOSS] train: 9.048, val: 4.708\n",
      "[ACC%] train: 94.81%, val: 10.34%\n",
      "EPOCH: 821\n",
      "[LOSS] train: 8.95, val: 4.652\n",
      "[ACC%] train: 97.11%, val: 12.64%\n",
      "EPOCH: 822\n",
      "[LOSS] train: 8.982, val: 4.71\n",
      "[ACC%] train: 96.39%, val: 10.34%\n",
      "EPOCH: 823\n",
      "[LOSS] train: 9.036, val: 4.7\n",
      "[ACC%] train: 95.53%, val: 10.92%\n",
      "EPOCH: 824\n",
      "[LOSS] train: 8.891, val: 4.626\n",
      "[ACC%] train: 97.98%, val: 10.92%\n",
      "EPOCH: 825\n",
      "[LOSS] train: 8.997, val: 4.679\n",
      "[ACC%] train: 96.25%, val: 10.34%\n",
      "EPOCH: 826\n",
      "[LOSS] train: 8.974, val: 4.701\n",
      "[ACC%] train: 96.25%, val: 10.34%\n",
      "EPOCH: 827\n",
      "[LOSS] train: 8.978, val: 4.666\n",
      "[ACC%] train: 96.25%, val: 12.07%\n",
      "EPOCH: 828\n",
      "[LOSS] train: 8.999, val: 4.734\n",
      "[ACC%] train: 95.96%, val: 10.34%\n",
      "EPOCH: 829\n",
      "[LOSS] train: 8.963, val: 4.724\n",
      "[ACC%] train: 96.68%, val: 9.77%\n",
      "EPOCH: 830\n",
      "[LOSS] train: 8.949, val: 4.664\n",
      "[ACC%] train: 96.83%, val: 13.79%\n",
      "EPOCH: 831\n",
      "[LOSS] train: 8.983, val: 4.675\n",
      "[ACC%] train: 95.96%, val: 12.64%\n",
      "EPOCH: 832\n",
      "[LOSS] train: 8.992, val: 4.666\n",
      "[ACC%] train: 96.25%, val: 11.49%\n",
      "EPOCH: 833\n",
      "[LOSS] train: 8.971, val: 4.713\n",
      "[ACC%] train: 96.39%, val: 10.34%\n",
      "EPOCH: 834\n",
      "[LOSS] train: 8.982, val: 4.707\n",
      "[ACC%] train: 96.1%, val: 8.05%\n",
      "EPOCH: 835\n",
      "[LOSS] train: 9.02, val: 4.693\n",
      "[ACC%] train: 95.82%, val: 11.49%\n",
      "EPOCH: 836\n",
      "[LOSS] train: 9.006, val: 4.645\n",
      "[ACC%] train: 95.82%, val: 13.22%\n",
      "EPOCH: 837\n",
      "[LOSS] train: 9.028, val: 4.659\n",
      "[ACC%] train: 95.53%, val: 12.07%\n",
      "EPOCH: 838\n",
      "[LOSS] train: 8.927, val: 4.739\n",
      "[ACC%] train: 97.4%, val: 7.47%\n",
      "EPOCH: 839\n",
      "[LOSS] train: 9.026, val: 4.679\n",
      "[ACC%] train: 96.25%, val: 8.62%\n",
      "EPOCH: 840\n",
      "[LOSS] train: 8.992, val: 4.724\n",
      "[ACC%] train: 96.54%, val: 9.77%\n",
      "EPOCH: 841\n",
      "[LOSS] train: 8.985, val: 4.698\n",
      "[ACC%] train: 96.25%, val: 11.49%\n",
      "EPOCH: 842\n",
      "[LOSS] train: 8.943, val: 4.696\n",
      "[ACC%] train: 96.83%, val: 11.49%\n",
      "EPOCH: 843\n",
      "[LOSS] train: 9.02, val: 4.731\n",
      "[ACC%] train: 95.53%, val: 8.62%\n",
      "EPOCH: 844\n",
      "[LOSS] train: 9.028, val: 4.658\n",
      "[ACC%] train: 95.96%, val: 11.49%\n",
      "EPOCH: 845\n",
      "[LOSS] train: 8.971, val: 4.636\n",
      "[ACC%] train: 96.68%, val: 12.64%\n",
      "EPOCH: 846\n",
      "[LOSS] train: 8.897, val: 4.726\n",
      "[ACC%] train: 97.84%, val: 10.34%\n",
      "EPOCH: 847\n",
      "[LOSS] train: 9.031, val: 4.71\n",
      "[ACC%] train: 95.67%, val: 10.34%\n",
      "EPOCH: 848\n",
      "[LOSS] train: 8.978, val: 4.726\n",
      "[ACC%] train: 96.39%, val: 9.77%\n",
      "EPOCH: 849\n",
      "[LOSS] train: 8.967, val: 4.72\n",
      "[ACC%] train: 96.83%, val: 11.49%\n",
      "EPOCH: 850\n",
      "[LOSS] train: 8.959, val: 4.741\n",
      "[ACC%] train: 96.68%, val: 8.62%\n",
      "EPOCH: 851\n",
      "[LOSS] train: 8.971, val: 4.719\n",
      "[ACC%] train: 96.54%, val: 8.62%\n",
      "EPOCH: 852\n",
      "[LOSS] train: 9.042, val: 4.737\n",
      "[ACC%] train: 95.67%, val: 10.34%\n",
      "EPOCH: 853\n",
      "[LOSS] train: 8.999, val: 4.702\n",
      "[ACC%] train: 96.1%, val: 12.64%\n",
      "EPOCH: 854\n",
      "[LOSS] train: 8.977, val: 4.653\n",
      "[ACC%] train: 95.82%, val: 13.22%\n",
      "EPOCH: 855\n",
      "[LOSS] train: 8.98, val: 4.69\n",
      "[ACC%] train: 96.68%, val: 12.07%\n",
      "EPOCH: 856\n",
      "[LOSS] train: 8.999, val: 4.587\n",
      "[ACC%] train: 95.96%, val: 16.09%\n",
      "EPOCH: 857\n",
      "[LOSS] train: 8.919, val: 4.641\n",
      "[ACC%] train: 97.26%, val: 14.37%\n",
      "EPOCH: 858\n",
      "[LOSS] train: 8.99, val: 4.629\n",
      "[ACC%] train: 96.54%, val: 13.22%\n",
      "EPOCH: 859\n",
      "[LOSS] train: 8.995, val: 4.687\n",
      "[ACC%] train: 96.39%, val: 10.92%\n",
      "EPOCH: 860\n",
      "[LOSS] train: 8.967, val: 4.671\n",
      "[ACC%] train: 96.83%, val: 12.64%\n",
      "EPOCH: 861\n",
      "[LOSS] train: 9.03, val: 4.682\n",
      "[ACC%] train: 95.96%, val: 11.49%\n",
      "EPOCH: 862\n",
      "[LOSS] train: 9.035, val: 4.652\n",
      "[ACC%] train: 95.38%, val: 12.07%\n",
      "EPOCH: 863\n",
      "[LOSS] train: 8.941, val: 4.651\n",
      "[ACC%] train: 97.4%, val: 12.64%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 864\n",
      "[LOSS] train: 8.972, val: 4.609\n",
      "[ACC%] train: 96.25%, val: 13.79%\n",
      "EPOCH: 865\n",
      "[LOSS] train: 8.94, val: 4.612\n",
      "[ACC%] train: 96.97%, val: 14.37%\n",
      "EPOCH: 866\n",
      "[LOSS] train: 8.917, val: 4.68\n",
      "[ACC%] train: 97.26%, val: 12.64%\n",
      "EPOCH: 867\n",
      "[LOSS] train: 9.043, val: 4.678\n",
      "[ACC%] train: 95.38%, val: 12.64%\n",
      "EPOCH: 868\n",
      "[LOSS] train: 8.958, val: 4.584\n",
      "[ACC%] train: 96.54%, val: 14.94%\n",
      "EPOCH: 869\n",
      "[LOSS] train: 8.957, val: 4.612\n",
      "[ACC%] train: 97.11%, val: 13.22%\n",
      "EPOCH: 870\n",
      "[LOSS] train: 8.993, val: 4.649\n",
      "[ACC%] train: 95.96%, val: 12.64%\n",
      "EPOCH: 871\n",
      "[LOSS] train: 8.932, val: 4.667\n",
      "[ACC%] train: 96.83%, val: 10.34%\n",
      "EPOCH: 872\n",
      "[LOSS] train: 8.903, val: 4.655\n",
      "[ACC%] train: 97.98%, val: 12.07%\n",
      "EPOCH: 873\n",
      "[LOSS] train: 8.923, val: 4.65\n",
      "[ACC%] train: 97.11%, val: 11.49%\n",
      "EPOCH: 874\n",
      "[LOSS] train: 8.974, val: 4.676\n",
      "[ACC%] train: 96.83%, val: 13.22%\n",
      "EPOCH: 875\n",
      "[LOSS] train: 8.935, val: 4.68\n",
      "[ACC%] train: 97.4%, val: 12.64%\n",
      "EPOCH: 876\n",
      "[LOSS] train: 8.913, val: 4.688\n",
      "[ACC%] train: 97.98%, val: 11.49%\n",
      "EPOCH: 877\n",
      "[LOSS] train: 9.009, val: 4.718\n",
      "[ACC%] train: 96.54%, val: 10.92%\n",
      "EPOCH: 878\n",
      "[LOSS] train: 8.942, val: 4.714\n",
      "[ACC%] train: 97.26%, val: 11.49%\n",
      "EPOCH: 879\n",
      "[LOSS] train: 9.018, val: 4.69\n",
      "[ACC%] train: 95.96%, val: 11.49%\n",
      "EPOCH: 880\n",
      "[LOSS] train: 8.889, val: 4.706\n",
      "[ACC%] train: 97.84%, val: 10.92%\n",
      "EPOCH: 881\n",
      "[LOSS] train: 8.998, val: 4.663\n",
      "[ACC%] train: 96.83%, val: 12.64%\n",
      "EPOCH: 882\n",
      "[LOSS] train: 8.935, val: 4.734\n",
      "[ACC%] train: 96.97%, val: 8.62%\n",
      "EPOCH: 883\n",
      "[LOSS] train: 9.012, val: 4.659\n",
      "[ACC%] train: 96.1%, val: 11.49%\n",
      "EPOCH: 884\n",
      "[LOSS] train: 8.999, val: 4.717\n",
      "[ACC%] train: 95.96%, val: 9.77%\n",
      "EPOCH: 885\n",
      "[LOSS] train: 9.082, val: 4.692\n",
      "[ACC%] train: 95.53%, val: 12.07%\n",
      "EPOCH: 886\n",
      "[LOSS] train: 8.995, val: 4.644\n",
      "[ACC%] train: 95.53%, val: 13.22%\n",
      "EPOCH: 887\n",
      "[LOSS] train: 9.039, val: 4.673\n",
      "[ACC%] train: 95.53%, val: 11.49%\n",
      "EPOCH: 888\n",
      "[LOSS] train: 9.049, val: 4.704\n",
      "[ACC%] train: 95.82%, val: 10.34%\n",
      "EPOCH: 889\n",
      "[LOSS] train: 9.141, val: 4.674\n",
      "[ACC%] train: 93.65%, val: 12.07%\n",
      "EPOCH: 890\n",
      "[LOSS] train: 9.119, val: 4.726\n",
      "[ACC%] train: 94.23%, val: 8.62%\n",
      "EPOCH: 891\n",
      "[LOSS] train: 9.122, val: 4.696\n",
      "[ACC%] train: 93.51%, val: 9.2%\n",
      "EPOCH: 892\n",
      "[LOSS] train: 9.096, val: 4.74\n",
      "[ACC%] train: 94.37%, val: 9.2%\n",
      "EPOCH: 893\n",
      "[LOSS] train: 9.141, val: 4.712\n",
      "[ACC%] train: 93.94%, val: 10.34%\n",
      "EPOCH: 894\n",
      "[LOSS] train: 9.145, val: 4.631\n",
      "[ACC%] train: 93.36%, val: 10.92%\n",
      "EPOCH: 895\n",
      "[LOSS] train: 9.12, val: 4.653\n",
      "[ACC%] train: 93.65%, val: 13.22%\n",
      "EPOCH: 896\n",
      "[LOSS] train: 9.077, val: 4.72\n",
      "[ACC%] train: 94.66%, val: 10.34%\n",
      "EPOCH: 897\n",
      "[LOSS] train: 9.014, val: 4.665\n",
      "[ACC%] train: 95.53%, val: 10.92%\n",
      "EPOCH: 898\n",
      "[LOSS] train: 9.02, val: 4.705\n",
      "[ACC%] train: 95.53%, val: 11.49%\n",
      "EPOCH: 899\n",
      "[LOSS] train: 9.025, val: 4.634\n",
      "[ACC%] train: 95.67%, val: 13.22%\n",
      "EPOCH: 900\n",
      "[LOSS] train: 9.005, val: 4.596\n",
      "[ACC%] train: 95.38%, val: 16.09%\n",
      "EPOCH: 901\n",
      "[LOSS] train: 9.033, val: 4.684\n",
      "[ACC%] train: 95.82%, val: 11.49%\n",
      "EPOCH: 902\n",
      "[LOSS] train: 9.128, val: 4.587\n",
      "[ACC%] train: 93.65%, val: 16.09%\n",
      "EPOCH: 903\n",
      "[LOSS] train: 9.092, val: 4.629\n",
      "[ACC%] train: 94.52%, val: 13.79%\n",
      "EPOCH: 904\n",
      "[LOSS] train: 9.024, val: 4.56\n",
      "[ACC%] train: 95.38%, val: 13.79%\n",
      "EPOCH: 905\n",
      "[LOSS] train: 9.038, val: 4.612\n",
      "[ACC%] train: 95.38%, val: 13.22%\n",
      "EPOCH: 906\n",
      "[LOSS] train: 9.057, val: 4.644\n",
      "[ACC%] train: 94.66%, val: 14.94%\n",
      "EPOCH: 907\n",
      "[LOSS] train: 8.916, val: 4.655\n",
      "[ACC%] train: 97.4%, val: 14.94%\n",
      "EPOCH: 908\n",
      "[LOSS] train: 8.985, val: 4.653\n",
      "[ACC%] train: 96.39%, val: 14.37%\n",
      "EPOCH: 909\n",
      "[LOSS] train: 8.961, val: 4.668\n",
      "[ACC%] train: 96.39%, val: 12.64%\n",
      "EPOCH: 910\n",
      "[LOSS] train: 8.985, val: 4.699\n",
      "[ACC%] train: 96.25%, val: 13.22%\n",
      "EPOCH: 911\n",
      "[LOSS] train: 9.015, val: 4.637\n",
      "[ACC%] train: 94.95%, val: 12.64%\n",
      "EPOCH: 912\n",
      "[LOSS] train: 8.949, val: 4.632\n",
      "[ACC%] train: 96.68%, val: 12.07%\n",
      "EPOCH: 913\n",
      "[LOSS] train: 9.005, val: 4.656\n",
      "[ACC%] train: 96.25%, val: 13.79%\n",
      "EPOCH: 914\n",
      "[LOSS] train: 9.097, val: 4.62\n",
      "[ACC%] train: 94.37%, val: 13.79%\n",
      "EPOCH: 915\n",
      "[LOSS] train: 9.106, val: 4.647\n",
      "[ACC%] train: 94.37%, val: 11.49%\n",
      "EPOCH: 916\n",
      "[LOSS] train: 9.091, val: 4.681\n",
      "[ACC%] train: 95.09%, val: 10.34%\n",
      "EPOCH: 917\n",
      "[LOSS] train: 9.109, val: 4.73\n",
      "[ACC%] train: 94.52%, val: 10.92%\n",
      "EPOCH: 918\n",
      "[LOSS] train: 9.004, val: 4.598\n",
      "[ACC%] train: 96.1%, val: 13.22%\n",
      "EPOCH: 919\n",
      "[LOSS] train: 9.072, val: 4.653\n",
      "[ACC%] train: 95.67%, val: 14.94%\n",
      "EPOCH: 920\n",
      "[LOSS] train: 9.13, val: 4.656\n",
      "[ACC%] train: 93.8%, val: 12.64%\n",
      "EPOCH: 921\n",
      "[LOSS] train: 9.121, val: 4.637\n",
      "[ACC%] train: 93.8%, val: 12.64%\n",
      "EPOCH: 922\n",
      "[LOSS] train: 9.128, val: 4.707\n",
      "[ACC%] train: 93.65%, val: 10.34%\n",
      "EPOCH: 923\n",
      "[LOSS] train: 8.963, val: 4.68\n",
      "[ACC%] train: 96.54%, val: 12.64%\n",
      "EPOCH: 924\n",
      "[LOSS] train: 8.983, val: 4.657\n",
      "[ACC%] train: 96.39%, val: 13.22%\n",
      "EPOCH: 925\n",
      "[LOSS] train: 9.011, val: 4.62\n",
      "[ACC%] train: 95.82%, val: 13.79%\n",
      "EPOCH: 926\n",
      "[LOSS] train: 9.09, val: 4.651\n",
      "[ACC%] train: 94.66%, val: 12.07%\n",
      "EPOCH: 927\n",
      "[LOSS] train: 9.001, val: 4.643\n",
      "[ACC%] train: 96.39%, val: 12.64%\n",
      "EPOCH: 928\n",
      "[LOSS] train: 8.988, val: 4.731\n",
      "[ACC%] train: 95.96%, val: 12.07%\n",
      "EPOCH: 929\n",
      "[LOSS] train: 9.003, val: 4.673\n",
      "[ACC%] train: 95.82%, val: 10.92%\n",
      "EPOCH: 930\n",
      "[LOSS] train: 8.947, val: 4.639\n",
      "[ACC%] train: 97.11%, val: 13.22%\n",
      "EPOCH: 931\n",
      "[LOSS] train: 9.074, val: 4.633\n",
      "[ACC%] train: 95.09%, val: 13.22%\n",
      "EPOCH: 932\n",
      "[LOSS] train: 9.047, val: 4.689\n",
      "[ACC%] train: 94.95%, val: 11.49%\n",
      "EPOCH: 933\n",
      "[LOSS] train: 9.061, val: 4.676\n",
      "[ACC%] train: 94.66%, val: 12.07%\n",
      "EPOCH: 934\n",
      "[LOSS] train: 9.154, val: 4.682\n",
      "[ACC%] train: 93.8%, val: 11.49%\n",
      "EPOCH: 935\n",
      "[LOSS] train: 9.095, val: 4.673\n",
      "[ACC%] train: 95.24%, val: 10.92%\n",
      "EPOCH: 936\n",
      "[LOSS] train: 9.047, val: 4.66\n",
      "[ACC%] train: 94.95%, val: 11.49%\n",
      "EPOCH: 937\n",
      "[LOSS] train: 9.009, val: 4.697\n",
      "[ACC%] train: 95.24%, val: 9.77%\n",
      "EPOCH: 938\n",
      "[LOSS] train: 9.154, val: 4.714\n",
      "[ACC%] train: 93.94%, val: 10.92%\n",
      "EPOCH: 939\n",
      "[LOSS] train: 9.189, val: 4.676\n",
      "[ACC%] train: 92.93%, val: 12.07%\n",
      "EPOCH: 940\n",
      "[LOSS] train: 9.102, val: 4.652\n",
      "[ACC%] train: 93.94%, val: 11.49%\n",
      "EPOCH: 941\n",
      "[LOSS] train: 9.158, val: 4.642\n",
      "[ACC%] train: 93.36%, val: 12.07%\n",
      "EPOCH: 942\n",
      "[LOSS] train: 9.079, val: 4.663\n",
      "[ACC%] train: 94.52%, val: 10.92%\n",
      "EPOCH: 943\n",
      "[LOSS] train: 9.076, val: 4.619\n",
      "[ACC%] train: 94.52%, val: 14.94%\n",
      "EPOCH: 944\n",
      "[LOSS] train: 9.076, val: 4.662\n",
      "[ACC%] train: 94.81%, val: 12.07%\n",
      "EPOCH: 945\n",
      "[LOSS] train: 9.115, val: 4.666\n",
      "[ACC%] train: 93.8%, val: 10.92%\n",
      "EPOCH: 946\n",
      "[LOSS] train: 9.036, val: 4.678\n",
      "[ACC%] train: 95.67%, val: 12.07%\n",
      "EPOCH: 947\n",
      "[LOSS] train: 9.037, val: 4.623\n",
      "[ACC%] train: 95.82%, val: 14.37%\n",
      "EPOCH: 948\n",
      "[LOSS] train: 9.051, val: 4.693\n",
      "[ACC%] train: 95.09%, val: 13.22%\n",
      "EPOCH: 949\n",
      "[LOSS] train: 9.049, val: 4.739\n",
      "[ACC%] train: 95.24%, val: 8.62%\n",
      "EPOCH: 950\n",
      "[LOSS] train: 8.94, val: 4.737\n",
      "[ACC%] train: 96.83%, val: 7.47%\n",
      "EPOCH: 951\n",
      "[LOSS] train: 9.056, val: 4.735\n",
      "[ACC%] train: 94.52%, val: 10.92%\n",
      "EPOCH: 952\n",
      "[LOSS] train: 8.996, val: 4.623\n",
      "[ACC%] train: 95.96%, val: 12.07%\n",
      "EPOCH: 953\n",
      "[LOSS] train: 8.985, val: 4.641\n",
      "[ACC%] train: 96.25%, val: 13.79%\n",
      "EPOCH: 954\n",
      "[LOSS] train: 9.027, val: 4.605\n",
      "[ACC%] train: 95.82%, val: 13.79%\n",
      "EPOCH: 955\n",
      "[LOSS] train: 9.064, val: 4.672\n",
      "[ACC%] train: 95.24%, val: 13.79%\n",
      "EPOCH: 956\n",
      "[LOSS] train: 9.049, val: 4.693\n",
      "[ACC%] train: 95.53%, val: 11.49%\n",
      "EPOCH: 957\n",
      "[LOSS] train: 8.969, val: 4.73\n",
      "[ACC%] train: 96.83%, val: 9.2%\n",
      "EPOCH: 958\n",
      "[LOSS] train: 9.014, val: 4.715\n",
      "[ACC%] train: 96.1%, val: 9.2%\n",
      "EPOCH: 959\n",
      "[LOSS] train: 9.02, val: 4.731\n",
      "[ACC%] train: 95.82%, val: 10.34%\n",
      "EPOCH: 960\n",
      "[LOSS] train: 8.995, val: 4.705\n",
      "[ACC%] train: 96.25%, val: 11.49%\n",
      "EPOCH: 961\n",
      "[LOSS] train: 9.072, val: 4.723\n",
      "[ACC%] train: 94.66%, val: 10.34%\n",
      "EPOCH: 962\n",
      "[LOSS] train: 8.964, val: 4.728\n",
      "[ACC%] train: 96.83%, val: 8.62%\n",
      "EPOCH: 963\n",
      "[LOSS] train: 9.017, val: 4.707\n",
      "[ACC%] train: 95.96%, val: 10.34%\n",
      "EPOCH: 964\n",
      "[LOSS] train: 9.035, val: 4.737\n",
      "[ACC%] train: 95.53%, val: 8.05%\n",
      "EPOCH: 965\n",
      "[LOSS] train: 9.043, val: 4.677\n",
      "[ACC%] train: 95.82%, val: 9.2%\n",
      "EPOCH: 966\n",
      "[LOSS] train: 8.98, val: 4.71\n",
      "[ACC%] train: 96.83%, val: 8.62%\n",
      "EPOCH: 967\n",
      "[LOSS] train: 8.967, val: 4.688\n",
      "[ACC%] train: 96.54%, val: 10.34%\n",
      "EPOCH: 968\n",
      "[LOSS] train: 9.052, val: 4.666\n",
      "[ACC%] train: 95.38%, val: 9.77%\n",
      "EPOCH: 969\n",
      "[LOSS] train: 8.994, val: 4.643\n",
      "[ACC%] train: 96.25%, val: 11.49%\n",
      "EPOCH: 970\n",
      "[LOSS] train: 8.986, val: 4.713\n",
      "[ACC%] train: 96.1%, val: 11.49%\n",
      "EPOCH: 971\n",
      "[LOSS] train: 8.998, val: 4.716\n",
      "[ACC%] train: 96.54%, val: 8.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 972\n",
      "[LOSS] train: 9.084, val: 4.739\n",
      "[ACC%] train: 94.37%, val: 9.2%\n",
      "EPOCH: 973\n",
      "[LOSS] train: 9.121, val: 4.675\n",
      "[ACC%] train: 93.94%, val: 12.07%\n",
      "EPOCH: 974\n",
      "[LOSS] train: 9.076, val: 4.685\n",
      "[ACC%] train: 94.95%, val: 9.77%\n",
      "EPOCH: 975\n",
      "[LOSS] train: 9.154, val: 4.708\n",
      "[ACC%] train: 93.51%, val: 9.77%\n",
      "EPOCH: 976\n",
      "[LOSS] train: 9.113, val: 4.788\n",
      "[ACC%] train: 94.23%, val: 6.9%\n",
      "EPOCH: 977\n",
      "[LOSS] train: 9.079, val: 4.752\n",
      "[ACC%] train: 94.52%, val: 9.77%\n",
      "EPOCH: 978\n",
      "[LOSS] train: 8.978, val: 4.661\n",
      "[ACC%] train: 96.39%, val: 13.22%\n",
      "EPOCH: 979\n",
      "[LOSS] train: 9.116, val: 4.64\n",
      "[ACC%] train: 94.23%, val: 12.07%\n",
      "EPOCH: 980\n",
      "[LOSS] train: 9.113, val: 4.647\n",
      "[ACC%] train: 94.08%, val: 13.79%\n",
      "EPOCH: 981\n",
      "[LOSS] train: 9.159, val: 4.608\n",
      "[ACC%] train: 93.51%, val: 13.22%\n",
      "EPOCH: 982\n",
      "[LOSS] train: 9.059, val: 4.673\n",
      "[ACC%] train: 95.24%, val: 13.22%\n",
      "EPOCH: 983\n",
      "[LOSS] train: 9.065, val: 4.619\n",
      "[ACC%] train: 94.52%, val: 12.07%\n",
      "EPOCH: 984\n",
      "[LOSS] train: 9.02, val: 4.636\n",
      "[ACC%] train: 95.96%, val: 13.22%\n",
      "EPOCH: 985\n",
      "[LOSS] train: 9.043, val: 4.718\n",
      "[ACC%] train: 95.24%, val: 9.2%\n",
      "EPOCH: 986\n",
      "[LOSS] train: 9.0, val: 4.727\n",
      "[ACC%] train: 95.67%, val: 9.77%\n",
      "EPOCH: 987\n",
      "[LOSS] train: 8.99, val: 4.707\n",
      "[ACC%] train: 96.25%, val: 9.77%\n",
      "EPOCH: 988\n",
      "[LOSS] train: 8.942, val: 4.633\n",
      "[ACC%] train: 97.11%, val: 12.07%\n",
      "EPOCH: 989\n",
      "[LOSS] train: 9.07, val: 4.717\n",
      "[ACC%] train: 94.81%, val: 9.77%\n",
      "EPOCH: 990\n",
      "[LOSS] train: 9.011, val: 4.734\n",
      "[ACC%] train: 95.67%, val: 10.34%\n",
      "EPOCH: 991\n",
      "[LOSS] train: 9.026, val: 4.679\n",
      "[ACC%] train: 95.82%, val: 10.34%\n",
      "EPOCH: 992\n",
      "[LOSS] train: 9.017, val: 4.707\n",
      "[ACC%] train: 95.82%, val: 11.49%\n",
      "EPOCH: 993\n",
      "[LOSS] train: 9.015, val: 4.645\n",
      "[ACC%] train: 95.67%, val: 11.49%\n",
      "EPOCH: 994\n",
      "[LOSS] train: 9.049, val: 4.777\n",
      "[ACC%] train: 95.24%, val: 8.05%\n",
      "EPOCH: 995\n",
      "[LOSS] train: 8.97, val: 4.685\n",
      "[ACC%] train: 96.1%, val: 10.92%\n",
      "EPOCH: 996\n",
      "[LOSS] train: 8.98, val: 4.712\n",
      "[ACC%] train: 96.1%, val: 11.49%\n",
      "EPOCH: 997\n",
      "[LOSS] train: 9.066, val: 4.711\n",
      "[ACC%] train: 95.96%, val: 9.77%\n",
      "EPOCH: 998\n",
      "[LOSS] train: 9.037, val: 4.729\n",
      "[ACC%] train: 95.96%, val: 9.77%\n",
      "EPOCH: 999\n",
      "[LOSS] train: 9.057, val: 4.749\n",
      "[ACC%] train: 95.24%, val: 9.77%\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "conv = ConvNet(ctype='multi')\n",
    "\n",
    "EPOCHS = range(1000)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor(multi_weight))\n",
    "optimizer = optim.Adam(conv.parameters(), lr=.001)\n",
    "\n",
    "for epoch in EPOCHS:\n",
    "    \n",
    "    train_batch_loss, train_batch_acc, train_cnt = 0, 0, 0\n",
    "    conv.train()\n",
    "    for x, y in train_multi_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = conv(x)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(\"Prediction: {}\".format(conv(x).argmax(axis=1)))\n",
    "        #print(\"True Value: {}\".format(y))\n",
    "        tmp_acc, tmp_t = acc_multi(y_pred, y)\n",
    "        train_batch_acc += tmp_acc\n",
    "        train_cnt += tmp_t\n",
    "        \n",
    "        train_batch_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(train_batch_loss)\n",
    "    train_accs.append(train_batch_acc)\n",
    "    \n",
    "    val_batch_loss, val_batch_acc, val_cnt = 0, 0, 0\n",
    "    conv.eval()\n",
    "    for x, y in test_multi_loader:\n",
    "        y_pred = conv(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        tmp_acc, tmp_t = acc_multi(y_pred, y)\n",
    "        val_batch_acc += tmp_acc\n",
    "        val_cnt += tmp_t\n",
    "        \n",
    "        val_batch_loss += loss.item()\n",
    "        #print(\"Prediction: {}\".format(y_pred.argmax(axis=1)))\n",
    "        #print(\"Prediction: {}\".format(y_pred[:3]))\n",
    "        \n",
    "    val_losses.append(val_batch_loss)\n",
    "    val_accs.append(val_batch_acc)\n",
    "    \n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    print(\"[LOSS] train: {}, val: {}\".format(round(train_batch_loss,3), round(val_batch_loss, 3)))\n",
    "    print(\"[ACC%] train: {}%, val: {}%\"\n",
    "          .format(round(train_batch_acc/train_cnt * 100, 2), round(val_batch_acc/val_cnt * 100, 2)))\n",
    "    #print(\"Prediction: {}\".format(conv(xx).argmax(axis=1)))\n",
    "    #print(\"True Value: {}\".format(yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T14:00:49.247334Z",
     "start_time": "2020-07-10T14:00:49.069828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Losses / epoch\")\n",
    "plt.plot(train_losses, label='Training Losses')\n",
    "plt.plot(val_losses, label='Validation Losses')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T14:00:49.823926Z",
     "start_time": "2020-07-10T14:00:49.687289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Accuracy / epoch\")\n",
    "plt.plot(np.array(train_accs)/train_cnt, label='Training Accuracy')\n",
    "plt.plot(np.array(val_accs)/val_cnt, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T03:41:52.506532Z",
     "start_time": "2020-07-11T03:41:52.331979Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pha\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 6, 1, 6, 7, 5, 0, 2, 6, 5, 2, 0, 8, 7, 8, 7, 6, 3, 8, 8, 3, 7, 5, 8,\n",
       "        4, 6, 2, 8, 3, 3, 3, 1, 8, 3, 1, 1, 2, 1, 4, 3, 1, 6, 2, 9, 0, 1],\n",
       "       grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(x).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T06:16:53.297065Z",
     "start_time": "2020-07-08T06:16:53.162321Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConvNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b90400406e19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ConvNet' is not defined"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "conv = ConvNet(ctype='binary')\n",
    "\n",
    "EPOCHS = range(100)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(conv.parameters(), lr=.001)\n",
    "\n",
    "for epoch in EPOCHS:\n",
    "    \n",
    "    train_batch_loss, train_batch_acc, train_cnt = 0, 0, 0\n",
    "    conv.train()\n",
    "    for x, y in train_binary_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = conv(x)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(\"Prediction: {}\".format(conv(x).argmax(axis=1)))\n",
    "        #print(\"True Value: {}\".format(y))\n",
    "        tmp_acc, tmp_t, tmp_cf = acc_binary(y_pred, y)\n",
    "        train_batch_acc += tmp_acc\n",
    "        train_cnt += tmp_t\n",
    "        \n",
    "        train_batch_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(train_batch_loss)\n",
    "    train_accs.append(train_batch_acc)\n",
    "    \n",
    "    val_batch_loss, val_batch_acc, val_cnt = 0, 0, 0\n",
    "    conv.eval()\n",
    "    for x, y in test_binary_loader:\n",
    "        y_pred = conv(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        tmp_acc, tmp_t, tmp_cf = acc_binary(y_pred, y)\n",
    "        val_batch_acc += tmp_acc\n",
    "        val_cnt += tmp_t\n",
    "        \n",
    "        val_batch_loss += loss.item()\n",
    "        \n",
    "    val_losses.append(val_batch_loss)\n",
    "    val_accs.append(val_batch_acc)\n",
    "    \n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    print(\"[LOSS] train: {}, val: {}\".format(round(train_batch_loss,3), round(val_batch_loss, 3)))\n",
    "    print(\"[ACC%] train: {}%, val: {}%\"\n",
    "          .format(round(train_batch_acc/train_cnt * 100, 2), round(val_batch_acc/val_cnt * 100, 2)))\n",
    "    #print(\"Prediction: {}\".format(conv(xx).argmax(axis=1)))\n",
    "    #print(\"True Value: {}\".format(yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T05:51:56.321973Z",
     "start_time": "2020-07-07T05:51:56.315216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1976)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([0.1, 0.2, 0.3, 0.9], dtype=torch.float)\n",
    "true = torch.tensor([0, 0, 0, 1], dtype=torch.float)\n",
    "loss_fn(pred, true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
